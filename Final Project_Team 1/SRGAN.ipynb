{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Set14 images\n",
      "Downloading Set14.zip 100.03%\n",
      "Extracting images\n",
      "Extracting 100.00%\n",
      "Set14 is all set!!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from urllib import request\n",
    "import os, sys\n",
    "\n",
    "path_set14 = r\"https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks/releases/download/v0.1/Set14.zip\"\n",
    "filename=\"Set14.zip\"\n",
    "def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            \n",
    "if not os.path.exists(\"tests/set14\"):\n",
    "    print(\"Downloading Set14 images\")\n",
    "    filehandler, _ = request.urlretrieve(path_set14, reporthook = _progress)\n",
    "    zf = zipfile.ZipFile(filehandler)\n",
    "    print()\n",
    "    print(\"Extracting images\")\n",
    "    uncompress_size = sum((file.file_size for file in zf.infolist()))\n",
    "\n",
    "    extracted_size = 0\n",
    "\n",
    "    for file in zf.infolist():\n",
    "        extracted_size += file.file_size\n",
    "        sys.stdout.write('\\rExtracting %.2f%%' % (float(extracted_size * 100/uncompress_size)))\n",
    "        sys.stdout.flush()\n",
    "        zf.extract(file, \"tests\")\n",
    "print()\n",
    "print(\"Set14 is all set!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Set5 images\n",
      "Downloading Set5 100.35%\n",
      "Extracting images\n",
      "Extracting 100.00%\n",
      "Set5 is all set!!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from urllib import request\n",
    "\n",
    "path_set5 = r\"https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks/releases/download/v0.1/Set5.zip\"\n",
    "filename=\"Set5\"\n",
    "def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "if not os.path.exists(\"tests/set5\"):\n",
    "    print(\"Downloading Set5 images\")\n",
    "    filehandler, _ = request.urlretrieve(path_set5, reporthook=_progress)\n",
    "    zf = zipfile.ZipFile(filehandler)\n",
    "    print()\n",
    "    print(\"Extracting images\")\n",
    "    uncompress_size = sum((file.file_size for file in zf.infolist()))\n",
    "\n",
    "    extracted_size = 0\n",
    "\n",
    "    for file in zf.infolist():\n",
    "        extracted_size += file.file_size\n",
    "        sys.stdout.write('\\rExtracting %.2f%%' % (float(extracted_size * 100/uncompress_size)))\n",
    "        sys.stdout.flush()\n",
    "        zf.extract(file, \"tests\")\n",
    "    \n",
    "print()\n",
    "print(\"Set5 is all set!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading BSD100 images\n",
      "Downloading bsd100.zip 100.06%\n",
      "Extracting images\n",
      "Extracting 100.00%\n",
      "BSD100 is all set\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from urllib import request\n",
    "\n",
    "path_bsd100 = r\"https://github.com/titu1994/Super-Resolution-using-Generative-Adversarial-Networks/releases/download/v0.1/bsd100.zip\"\n",
    "filename=\"bsd100.zip\"\n",
    "def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "if not os.path.exists(\"tests/set14\"):\n",
    "    print(\"Downloading BSD100 images\")\n",
    "    filehandler, _ = request.urlretrieve(path_bsd100, reporthook=_progress)\n",
    "    zf = zipfile.ZipFile(filehandler)\n",
    "    print()\n",
    "\n",
    "    print(\"Extracting images\")\n",
    "    uncompress_size = sum((file.file_size for file in zf.infolist()))\n",
    "\n",
    "    extracted_size = 0\n",
    "\n",
    "    for file in zf.infolist():\n",
    "        extracted_size += file.file_size\n",
    "        sys.stdout.write('\\rExtracting %.2f%%' % (float(extracted_size * 100/uncompress_size)))\n",
    "        sys.stdout.flush()\n",
    "        zf.extract(file, \"tests\")\n",
    "print()\n",
    "print(\"BSD100 is all set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Coco images\n",
      "Downloading train2014 100.00%\n",
      "Extracting images\n",
      "Extracting 100.00%Downloading Coco annotations\n",
      "Downloading annotations 100.00%\n",
      "Extracting annotations\n",
      "Extracting 100.00%\n",
      "Coco is all set!!\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from urllib import request\n",
    "import os, sys\n",
    "\n",
    "path_coco = r\"http://images.cocodataset.org/zips/train2014.zip\"\n",
    "filename=\"train2014\"\n",
    "def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
    "                float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "if not os.path.exists(\"tests\\coco\"):\n",
    "    print(\"Downloading Coco images\")\n",
    "    filehandler, _ = request.urlretrieve(path_coco, reporthook=_progress)\n",
    "    zf = zipfile.ZipFile(filehandler)\n",
    "    uncompress_size = sum((file.file_size for file in zf.infolist()))\n",
    "\n",
    "    extracted_size = 0\n",
    "    print()\n",
    "    print(\"Extracting images\")\n",
    "\n",
    "    for file in zf.infolist():\n",
    "        extracted_size += file.file_size\n",
    "        sys.stdout.write('\\rExtracting %.2f%%' % (float(extracted_size * 100/uncompress_size)))\n",
    "        sys.stdout.flush()\n",
    "        zf.extract(file, \"tests/coco\")\n",
    "\n",
    "    os.rename(\"tests/coco/train2014\", \"tests/coco/images\")\n",
    "\n",
    "    filename=\"annotations\"\n",
    "    path_cocoann = r\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
    "\n",
    "    print(\"\\nDownloading Coco annotations\")\n",
    "    filehandler, _ = request.urlretrieve(path_cocoann, reporthook=_progress)\n",
    "\n",
    "    zf = zipfile.ZipFile(filehandler)\n",
    "    print()\n",
    "    print(\"Extracting annotations\")\n",
    "    uncompress_size = sum((file.file_size for file in zf.infolist()))\n",
    "\n",
    "    extracted_size = 0\n",
    "\n",
    "    for file in zf.infolist():\n",
    "        extracted_size += file.file_size\n",
    "        sys.stdout.write('\\rExtracting %.2f%%' % (float(extracted_size * 100/uncompress_size)))\n",
    "        sys.stdout.flush()\n",
    "        zf.extract(file, \"tests/coco\")\n",
    "print()\n",
    "print(\"Coco is all set!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\stable\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, add, concatenate, BatchNormalization, LeakyReLU, Flatten, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from keras_ops import fit as bypass_fit, smooth_gan_labels\n",
    "\n",
    "from layers import Normalize, Denormalize, SubPixelUpscaling\n",
    "from loss import AdversarialLossRegularizer, ContentVGGRegularizer, TVRegularizer, psnr, dummy_loss\n",
    "\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "from imageio import imwrite as imsave\n",
    "from skimage.transform import warp as imresize,resize\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "TF_WEIGHTS_PATH_NO_TOP = r\"https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "if not os.path.exists(\"weights/\"):\n",
    "    os.makedirs(\"weights/\")\n",
    "\n",
    "if not os.path.exists(\"val_images/\"):\n",
    "    os.makedirs(\"val_images/\")\n",
    "\n",
    "if K.image_dim_ordering() == \"th\":\n",
    "    channel_axis = 1\n",
    "else:\n",
    "    channel_axis = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNetwork:\n",
    "    '''\n",
    "    Helper class to load VGG and its weights to the FastNet model\n",
    "    '''\n",
    "\n",
    "    def __init__(self, img_width=384, img_height=384, vgg_weight=1.0):\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.vgg_weight = vgg_weight\n",
    "\n",
    "        self.vgg_layers = None\n",
    "\n",
    "    def append_vgg_network(self, x_in, true_X_input, pre_train=False):\n",
    "\n",
    "        # Append the initial inputs to the outputs of the SRResNet\n",
    "        x = concatenate([x_in, true_X_input], axis=0)\n",
    "\n",
    "        # Normalize the inputs via custom VGG Normalization layer\n",
    "        x = Normalize(name=\"normalize_vgg\")(x)\n",
    "\n",
    "        # Begin adding the VGG layers\n",
    "        x = Conv2D(64, (3, 3), activation='relu', name='vgg_conv1_1', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "\n",
    "        x = Conv2D(64, (3, 3), activation='relu', name='vgg_conv1_2', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool1')(x)\n",
    "\n",
    "        x = Conv2D(128, (3, 3), activation='relu', name='vgg_conv2_1', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "\n",
    "        if pre_train:\n",
    "            vgg_regularizer2 = ContentVGGRegularizer(weight=self.vgg_weight)\n",
    "            x = Conv2D(128, (3, 3), activation='relu', name='vgg_conv2_2', padding='same',\n",
    "                              activity_regularizer=vgg_regularizer2, kernel_initializer=\"glorot_uniform\")(x)\n",
    "        else:\n",
    "            x = Conv2D(128, (3, 3), activation='relu', name='vgg_conv2_2', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool2')(x)\n",
    "\n",
    "        x = Conv2D(256, (3, 3), activation='relu', name='vgg_conv3_1', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', name='vgg_conv3_2', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "\n",
    "        x = Conv2D(256, (3, 3), activation='relu', name='vgg_conv3_3', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool3')(x)\n",
    "\n",
    "        x = Conv2D(512, (3, 3), activation='relu', name='vgg_conv4_1', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', name='vgg_conv4_2', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "\n",
    "        x = Conv2D(512, (3, 3), activation='relu', name='vgg_conv4_3', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool4')(x)\n",
    "\n",
    "        x = Conv2D(512, (3, 3), activation='relu', name='vgg_conv5_1', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', name='vgg_conv5_2', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "\n",
    "        if not pre_train:\n",
    "            vgg_regularizer5 = ContentVGGRegularizer(weight=self.vgg_weight)\n",
    "            x = Conv2D(512, (3, 3), activation='relu', name='vgg_conv5_3', padding='same',\n",
    "                          activity_regularizer=vgg_regularizer5, kernel_initializer=\"glorot_uniform\")(x)\n",
    "        else:\n",
    "            x = Conv2D(512, (3, 3), activation='relu', name='vgg_conv5_3', padding='same', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool5')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def load_vgg_weight(self, model):\n",
    "        # Loading VGG 16 weights\n",
    "        if K.image_dim_ordering() == \"th\":\n",
    "            weights = get_file('vgg16_weights_th_dim_ordering_th_kernels_notop.h5', THEANO_WEIGHTS_PATH_NO_TOP,\n",
    "                                   cache_subdir='models')\n",
    "        else:\n",
    "            weights = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', TF_WEIGHTS_PATH_NO_TOP,\n",
    "                                   cache_subdir='models')\n",
    "        f = h5py.File(weights)\n",
    "\n",
    "        layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "        if self.vgg_layers is None:\n",
    "            self.vgg_layers = [layer for layer in model.layers\n",
    "                               if 'vgg_' in layer.name]\n",
    "\n",
    "        for i, layer in enumerate(self.vgg_layers):\n",
    "            g = f[layer_names[i]]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "\n",
    "        # Freeze all VGG layers\n",
    "        for layer in self.vgg_layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNetwork:\n",
    "\n",
    "    def __init__(self, img_width=384, img_height=384, adversarial_loss_weight=1, small_model=False):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.adversarial_loss_weight = adversarial_loss_weight\n",
    "        self.small_model = small_model\n",
    "\n",
    "        self.k = 3\n",
    "        self.mode = 2\n",
    "        self.weights_path = \"weights/Discriminator weights.h5\"\n",
    "\n",
    "        self.gan_layers = None\n",
    "\n",
    "    def append_gan_network(self, true_X_input):\n",
    "\n",
    "        # Normalize the inputs via custom VGG Normalization layer\n",
    "        x = Normalize(type=\"gan\", value=127.5, name=\"gan_normalize\")(true_X_input)\n",
    "\n",
    "        x = Conv2D(64, (self.k, self.k), padding='same', name='gan_conv1_1', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = LeakyReLU(0.3, name=\"gan_lrelu1_1\")(x)\n",
    "\n",
    "        x = Conv2D(64, (self.k, self.k), padding='same', name='gan_conv1_2', strides=(2, 2), kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = LeakyReLU(0.3, name='gan_lrelu1_2')(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='gan_batchnorm1_1')(x)\n",
    "\n",
    "        filters = [128, 256] if self.small_model else [128, 256, 512]\n",
    "\n",
    "        for i, num_filters in enumerate(filters):\n",
    "            for j in range(2):\n",
    "                strides = (2, 2) if j == 1 else (1, 1)\n",
    "            \n",
    "                x = Conv2D(num_filters, (self.k, self.k), padding='same', strides=strides,\n",
    "                                  name='gan_conv%d_%d' % (i + 2, j + 1), kernel_initializer=\"glorot_uniform\")(x)\n",
    "                x = LeakyReLU(0.3, name='gan_lrelu_%d_%d' % (i + 2, j + 1))(x)\n",
    "                x = BatchNormalization(axis=channel_axis, name='gan_batchnorm%d_%d' % (i + 2, j + 1))(x)\n",
    "\n",
    "        x = Flatten(name='gan_flatten')(x)\n",
    "\n",
    "        output_dim = 128 if self.small_model else 1024\n",
    "\n",
    "        x = Dense(output_dim, name='gan_dense1')(x)\n",
    "        x = LeakyReLU(0.3, name='gan_lrelu5')(x)\n",
    "\n",
    "        gan_regulrizer = AdversarialLossRegularizer(weight=self.adversarial_loss_weight)\n",
    "        x = Dense(2, activation=\"softmax\", activity_regularizer=gan_regulrizer, name='gan_output')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_trainable(self, model, value=True):\n",
    "        if self.gan_layers is None:\n",
    "            disc_model = [layer for layer in model.layers\n",
    "                          if 'model' in layer.name][0] # Only disc model is an inner model\n",
    "\n",
    "            self.gan_layers = [layer for layer in disc_model.layers\n",
    "                               if 'gan_' in layer.name]\n",
    "\n",
    "        for layer in self.gan_layers:\n",
    "            layer.trainable = value\n",
    "\n",
    "    def load_gan_weights(self, model):\n",
    "        f = h5py.File(self.weights_path)\n",
    "\n",
    "        layer_names = [name for name in f.attrs['layer_names']]\n",
    "        layer_names = layer_names[1:] # First is an input layer. Not needed.\n",
    "\n",
    "        if self.gan_layers is None:\n",
    "            self.gan_layers = [layer for layer in model.layers\n",
    "                                if 'gan_' in layer.name]\n",
    "\n",
    "        for i, layer in enumerate(self.gan_layers):\n",
    "            g = f[layer_names[i]]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "\n",
    "        print(\"GAN Model weights loaded.\")\n",
    "        return model\n",
    "\n",
    "    def save_gan_weights(self, model):\n",
    "        print('GAN Weights are being saved.')\n",
    "        model.save_weights(self.weights_path, overwrite=True)\n",
    "        print('GAN Weights saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeNetwork:\n",
    "\n",
    "    def __init__(self, img_width=96, img_height=96, batch_size=16, num_upscales=2, small_model=False,\n",
    "                 content_weight=1, tv_weight=2e5, gen_channels=64):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "        self.small_model = small_model\n",
    "        self.num_scales = num_upscales\n",
    "\n",
    "        self.content_weight = content_weight\n",
    "        self.tv_weight = tv_weight\n",
    "\n",
    "        self.filters = gen_channels\n",
    "        self.mode = 2\n",
    "        self.init = 'glorot_uniform'\n",
    "\n",
    "        self.sr_res_layers = None\n",
    "        self.sr_weights_path = \"weights/SRGAN.h5\"\n",
    "\n",
    "        self.output_func = None\n",
    "\n",
    "    def create_sr_model(self, ip):\n",
    "\n",
    "        x = Conv2D(self.filters, (5, 5), activation='linear', padding='same', name='sr_res_conv1',\n",
    "                          kernel_initializer=self.init)(ip)\n",
    "        x = BatchNormalization(axis=channel_axis, name='sr_res_bn_1')(x)\n",
    "        x = LeakyReLU(alpha=0.25, name='sr_res_lr1')(x)\n",
    "\n",
    "        x = Conv2D(self.filters, (5, 5), activation='linear', padding='same', name='sr_res_conv2', kernel_initializer=\"glorot_uniform\")(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='sr_res_bn_2')(x)\n",
    "        x = LeakyReLU(alpha=0.25, name='sr_res_lr2')(x)\n",
    "\n",
    "        num_residual = 5 if self.small_model else 15\n",
    "\n",
    "        for i in range(num_residual):\n",
    "            x = self._residual_block(x, i + 1)\n",
    "\n",
    "        for scale in range(self.num_scales):\n",
    "            x = self._upscale_block(x, scale + 1)\n",
    "    \n",
    "        scale = 2 ** self.num_scales\n",
    "        tv_regularizer = TVRegularizer(img_width=self.img_width * scale, img_height=self.img_height * scale,\n",
    "                                       weight=self.tv_weight) #self.tv_weight)\n",
    "        \n",
    "        x = Conv2D(3, (5, 5), activation='tanh', padding='same', activity_regularizer=tv_regularizer, \n",
    "                   name='sr_res_conv_final', kernel_initializer=self.init)(x)\n",
    "        \n",
    "        x = Denormalize(name='sr_res_conv_denorm')(x)\n",
    "        return x\n",
    "\n",
    "    def _residual_block(self, ip, id):\n",
    "        init = ip\n",
    "\n",
    "        x = Conv2D(self.filters, (3, 3), activation='linear', padding='same', name='sr_res_conv_' + str(id) + '_1',\n",
    "                          kernel_initializer=self.init)(ip)\n",
    "        x = BatchNormalization(axis=channel_axis, name='sr_res_bn_' + str(id) + '_1')(x)\n",
    "        x = LeakyReLU(alpha=0.25, name=\"sr_res_activation_\" + str(id) + \"_1\")(x)\n",
    "\n",
    "        x = Conv2D(self.filters, (3, 3), activation='linear', padding='same', name='sr_res_conv_' + str(id) + '_2',\n",
    "                          kernel_initializer=self.init)(x)\n",
    "        x = BatchNormalization(axis=channel_axis, name='sr_res_bn_' + str(id) + '_2')(x)\n",
    "\n",
    "        m = add([x, init],name=\"sr_res_merge_\" + str(id))\n",
    "\n",
    "        return m\n",
    "\n",
    "    def _upscale_block(self, ip, id):\n",
    "        '''\n",
    "        As per suggestion from http://distill.pub/2016/deconv-checkerboard/, I am swapping out\n",
    "        SubPixelConvolution to simple Nearest Neighbour Upsampling\n",
    "        '''\n",
    "        init = ip\n",
    "        \n",
    "        x = Conv2D(128, (3, 3), activation=\"linear\", padding='same', name='sr_res_upconv1_%d' % id,\n",
    "                          kernel_initializer=self.init)(init)\n",
    "        x = LeakyReLU(alpha=0.25, name='sr_res_up_lr_%d_1_1' % id)(x)\n",
    "        x = UpSampling2D(name='sr_res_upscale_%d' % id)(x)\n",
    "        #x = SubPixelUpscaling(r=2, channels=32)(x)\n",
    "        x = Conv2D(128, (3, 3), activation=\"linear\", padding='same', name='sr_res_filter1_%d' % id,\n",
    "                          kernel_initializer=self.init)(x)\n",
    "        x = LeakyReLU(alpha=0.3, name='sr_res_up_lr_%d_1_2' % id)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_trainable(self, model, value=True):\n",
    "        if self.sr_res_layers is None:\n",
    "            self.sr_res_layers = [layer for layer in model.layers\n",
    "                                    if 'sr_res_' in layer.name]\n",
    "\n",
    "        for layer in self.sr_res_layers:\n",
    "            layer.trainable = value\n",
    "\n",
    "    def get_generator_output(self, input_img, srgan_model):\n",
    "        if self.output_func is None:\n",
    "            gen_output_layer = [layer for layer in srgan_model.layers\n",
    "                                if layer.name == \"sr_res_conv_denorm\"][0]\n",
    "            self.output_func = K.function([srgan_model.layers[0].input],\n",
    "                                          [gen_output_layer.output])\n",
    "\n",
    "        return self.output_func([input_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGANNetwork:\n",
    "\n",
    "    def __init__(self, img_width=96, img_height=96, batch_size=16, num_scales=2):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "        self.num_scales = num_scales\n",
    "\n",
    "        self.discriminative_network = None # type: DiscriminatorNetwork\n",
    "        self.generative_network = None # type: GenerativeNetwork\n",
    "        self.vgg_network = None # type: VGGNetwork\n",
    "\n",
    "        self.srgan_model_ = None # type: Model\n",
    "        self.generative_model_ = None # type: Model\n",
    "        self.discriminative_model_ = None #type: Model\n",
    "\n",
    "    def build_srgan_pretrain_model(self, use_small_srgan=False):\n",
    "        large_width = self.img_width * 4\n",
    "        large_height = self.img_height * 4\n",
    "\n",
    "        self.generative_network = GenerativeNetwork(self.img_width, self.img_height, self.batch_size, self.num_scales,\n",
    "                                                    use_small_srgan)\n",
    "        self.vgg_network = VGGNetwork(large_width, large_height)\n",
    "\n",
    "        ip = Input(shape=(self.img_width, self.img_height, 3), name='x_generator')\n",
    "        ip_vgg = Input(shape=(large_width, large_height, 3), name='x_vgg')  # Actual X images\n",
    "\n",
    "        sr_output = self.generative_network.create_sr_model(ip)\n",
    "        self.generative_model_ = Model(ip, sr_output)\n",
    "\n",
    "        vgg_output = self.vgg_network.append_vgg_network(sr_output, ip_vgg, pre_train=True)\n",
    "\n",
    "        self.srgan_model_ = Model(inputs=[ip, ip_vgg],\n",
    "                                  outputs=vgg_output)\n",
    "\n",
    "        self.vgg_network.load_vgg_weight(self.srgan_model_)\n",
    "\n",
    "        srgan_optimizer = Adam(lr=1e-4)\n",
    "        generator_optimizer = Adam(lr=1e-4)\n",
    "\n",
    "        self.generative_model_.compile(generator_optimizer, dummy_loss)\n",
    "        self.srgan_model_.compile(srgan_optimizer, dummy_loss)\n",
    "\n",
    "        return self.srgan_model_\n",
    "\n",
    "\n",
    "    def build_discriminator_pretrain_model(self, use_small_srgan=False, use_small_discriminator=False):\n",
    "        large_width = self.img_width * 4\n",
    "        large_height = self.img_height * 4\n",
    "\n",
    "        self.generative_network = GenerativeNetwork(self.img_width, self.img_height, self.batch_size, self.num_scales,\n",
    "                                                    use_small_srgan)\n",
    "        self.discriminative_network = DiscriminatorNetwork(large_width, large_height,\n",
    "                                                           small_model=use_small_discriminator)\n",
    "\n",
    "        ip = Input(shape=(self.img_width, self.img_height, 3), name='x_generator')\n",
    "        ip_gan = Input(shape=(large_width, large_height, 3), name='x_discriminator')  # Actual X images\n",
    "\n",
    "        sr_output = self.generative_network.create_sr_model(ip)\n",
    "        self.generative_model_ = Model(ip, sr_output)\n",
    "        #self.generative_network.set_trainable(self.generative_model_, value=False)\n",
    "\n",
    "        gan_output = self.discriminative_network.append_gan_network(ip_gan)\n",
    "        self.discriminative_model_ = Model(ip_gan, gan_output)\n",
    "\n",
    "        generator_out = self.generative_model_(ip)\n",
    "        gan_output = self.discriminative_model_(generator_out)\n",
    "\n",
    "        self.srgan_model_ = Model(inputs=ip, outputs=gan_output)\n",
    "\n",
    "        srgan_optimizer = Adam(lr=1e-4)\n",
    "        generator_optimizer = Adam(lr=1e-4)\n",
    "        discriminator_optimizer = Adam(lr=1e-4)\n",
    "\n",
    "        self.generative_model_.compile(generator_optimizer, loss='mse')\n",
    "        self.discriminative_model_.compile(discriminator_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "        self.srgan_model_.compile(srgan_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "        return self.discriminative_model_\n",
    "\n",
    "\n",
    "    def build_srgan_model(self, use_small_srgan=False, use_small_discriminator=False):\n",
    "        large_width = self.img_width * 4\n",
    "        large_height = self.img_height * 4\n",
    "\n",
    "        self.generative_network = GenerativeNetwork(self.img_width, self.img_height, self.batch_size, num_upscales=self.num_scales,\n",
    "                                                    small_model=use_small_srgan)\n",
    "        self.discriminative_network = DiscriminatorNetwork(large_width, large_height,\n",
    "                                                           small_model=use_small_discriminator)\n",
    "        self.vgg_network = VGGNetwork(large_width, large_height)\n",
    "\n",
    "        ip = Input(shape=(self.img_width, self.img_height, 3), name='x_generator')\n",
    "        ip_gan = Input(shape=(large_width, large_height, 3), name='x_discriminator') # Actual X images\n",
    "        ip_vgg = Input(shape=(large_width, large_height, 3), name='x_vgg') # Actual X images\n",
    "        sr_output = self.generative_network.create_sr_model(ip)\n",
    "        self.generative_model_ = Model(ip, sr_output)\n",
    "\n",
    "        gan_output = self.discriminative_network.append_gan_network(ip_gan)\n",
    "        self.discriminative_model_ = Model(ip_gan, gan_output)        \n",
    "        gan_output = self.discriminative_model_(self.generative_model_.output)\n",
    "        vgg_output = self.vgg_network.append_vgg_network(self.generative_model_.output, ip_vgg)\n",
    "\n",
    "        self.srgan_model_ = Model(inputs=[ip, ip_gan, ip_vgg], outputs=[gan_output, vgg_output])\n",
    "\n",
    "        self.vgg_network.load_vgg_weight(self.srgan_model_)\n",
    "\n",
    "        srgan_optimizer = Adam(lr=1e-4)\n",
    "        generator_optimizer = Adam(lr=1e-4)\n",
    "        discriminator_optimizer = Adam(lr=1e-4)\n",
    "\n",
    "        self.generative_model_.compile(generator_optimizer, dummy_loss)\n",
    "        self.discriminative_model_.compile(discriminator_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "        self.srgan_model_.compile(srgan_optimizer, dummy_loss)\n",
    "\n",
    "        return self.srgan_model_\n",
    "\n",
    "\n",
    "    def pre_train_srgan(self, image_dir, num_images=50000, epochs=1, use_small_srgan=False):\n",
    "        self.build_srgan_pretrain_model(use_small_srgan=use_small_srgan)\n",
    "\n",
    "        self._train_model(image_dir, num_images=num_images, epochs=epochs, pre_train_srgan=True,\n",
    "                          load_generative_weights=True)\n",
    "\n",
    "    def pre_train_discriminator(self, image_dir, num_images=50000, epochs=1, batch_size=128,\n",
    "                                use_small_discriminator=False):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.build_discriminator_pretrain_model(use_small_discriminator)\n",
    "\n",
    "        self._train_model(image_dir, num_images, epochs, pre_train_discriminator=True,\n",
    "                          load_generative_weights=True)\n",
    "\n",
    "    def train_full_model(self, image_dir, num_images=50000, epochs=10, use_small_srgan=False,\n",
    "                         use_small_discriminator=False):\n",
    "\n",
    "        self.build_srgan_model(use_small_srgan, use_small_discriminator)\n",
    "\n",
    "        self._train_model(image_dir, num_images, epochs, load_generative_weights=True, load_discriminator_weights=True)\n",
    "\n",
    "    def _train_model(self, image_dir, num_images=80000, epochs=10, pre_train_srgan=False,\n",
    "                     pre_train_discriminator=False, load_generative_weights=False, load_discriminator_weights=False,\n",
    "                     save_loss=True, disc_train_flip=0.1):\n",
    "\n",
    "        assert self.img_width >= 16, \"Minimum image width must be at least 16\"\n",
    "        assert self.img_height >= 16, \"Minimum image height must be at least 16\"\n",
    "\n",
    "        if load_generative_weights:\n",
    "            try:\n",
    "                self.generative_model_.load_weights(self.generative_network.sr_weights_path)\n",
    "                print(\"Generator weights loaded.\")\n",
    "            except:\n",
    "                print(\"Could not load generator weights.\")\n",
    "\n",
    "        if load_discriminator_weights:\n",
    "            try:\n",
    "                self.discriminative_network.load_gan_weights(self.srgan_model_)\n",
    "                print(\"Discriminator weights loaded.\")\n",
    "            except:\n",
    "                print(\"Could not load discriminator weights.\")\n",
    "\n",
    "        datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        img_width = self.img_width * 4\n",
    "        img_height = self.img_height * 4\n",
    "\n",
    "        early_stop = False\n",
    "        iteration = 0\n",
    "        prev_improvement = -1\n",
    "\n",
    "        if save_loss:\n",
    "            if pre_train_srgan:\n",
    "                loss_history = {'generator_loss' : [],\n",
    "                                'val_psnr' : [], }\n",
    "            elif pre_train_discriminator:\n",
    "                loss_history = {'discriminator_loss' : [],\n",
    "                                'discriminator_acc' : [], }\n",
    "            else:\n",
    "                loss_history = {'discriminator_loss' : [],\n",
    "                                'discriminator_acc' : [],\n",
    "                                'generator_loss' : [],\n",
    "                                'val_psnr': [], }\n",
    "\n",
    "        y_vgg_dummy = np.zeros((self.batch_size * 2, 3, img_width // 32, img_height // 32)) # 5 Max Pools = 2 ** 5 = 32\n",
    "\n",
    "        print(\"Training SRGAN network\")\n",
    "        for i in range(epochs):\n",
    "            print()\n",
    "            print(\"Epoch : %d\" % (i + 1))\n",
    "            for x in datagen.flow_from_directory(image_dir, class_mode=None, batch_size=self.batch_size,\n",
    "                                                 target_size=(img_width, img_height)):\n",
    "                try:\n",
    "                    t1 = time.time()\n",
    "\n",
    "                    if not pre_train_srgan and not pre_train_discriminator:\n",
    "                        x_vgg = x.copy() * 255 # VGG input [0 - 255 scale]\n",
    "\n",
    "                    # resize images\n",
    "                    x_temp = x.copy()\n",
    "                    x_temp = x_temp.transpose((0, 2, 3, 1))\n",
    "\n",
    "                    x_generator = np.empty((self.batch_size, self.img_width, self.img_height, 3))\n",
    "                    def shift_down(xy):                        \n",
    "                        return xy\n",
    "                    \n",
    "                    for j in range(self.batch_size):\n",
    "                        img = gaussian_filter(x_temp[j], sigma=0.1)\n",
    "                        img = imresize(img, inverse_map=shift_down, output_shape=(self.img_width, self.img_height, 3), order=3)\n",
    "                        #img = resize(img, (self.img_width, self.img_height))\n",
    "                        x_generator[j, :, :, :] = img\n",
    "\n",
    "                    #x_generator = x_generator.transpose((0, 3, 1, 2))\n",
    "\n",
    "                    if iteration % 50 == 0 and iteration != 0 and not pre_train_discriminator:\n",
    "                        print(\"Validation image..\")\n",
    "                        output_image_batch = self.generative_network.get_generator_output(x_generator,\n",
    "                                                                                          self.srgan_model_)\n",
    "                        if type(output_image_batch) == list:\n",
    "                            output_image_batch = output_image_batch[0]\n",
    "\n",
    "                        mean_axis = (0, 2, 3) if K.image_dim_ordering() == 'th' else (0, 1, 2)\n",
    "\n",
    "                        average_psnr = 0.0\n",
    "\n",
    "                        print('gen img mean :', np.mean(output_image_batch / 255., axis=mean_axis))\n",
    "                        print('val img mean :', np.mean(x, axis=mean_axis))\n",
    "\n",
    "                        for x_i in range(self.batch_size):\n",
    "                            average_psnr += psnr(x[x_i], np.clip(output_image_batch[x_i], 0, 255) / 255.)\n",
    "\n",
    "                        average_psnr /= self.batch_size\n",
    "\n",
    "                        if save_loss:\n",
    "                            loss_history['val_psnr'].append(average_psnr)\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "\n",
    "                        print(\"Time required : %0.2f. Average validation PSNR over %d samples = %0.2f\" %\n",
    "                              (t2 - t1, self.batch_size, average_psnr))\n",
    "\n",
    "                        for x_i in range(self.batch_size):\n",
    "                            real_path = \"val_images/epoch_%d_iteration_%d_num_%d_real_.png\" % (i + 1, iteration, x_i + 1)\n",
    "                            generated_path = \"val_images/epoch_%d_iteration_%d_num_%d_generated.png\" % (i + 1,\n",
    "                                                                                                        iteration,\n",
    "                                                                                                        x_i + 1)\n",
    "\n",
    "                            val_x = x[x_i].copy() * 255.\n",
    "                            val_x = val_x.transpose((1, 2, 0))\n",
    "                            val_x = np.clip(val_x, 0, 255).astype('uint8')\n",
    "\n",
    "                            output_image = output_image_batch[x_i]\n",
    "                            output_image = output_image.transpose((1, 2, 0))\n",
    "                            output_image = np.clip(output_image, 0, 255).astype('uint8')\n",
    "\n",
    "                            imsave(real_path, val_x[:,:,0])\n",
    "                            imsave(generated_path, output_image[:,:,0])\n",
    "\n",
    "                        '''\n",
    "                        Don't train of validation images for now.\n",
    "\n",
    "                        Note that if epochs > 1, there is a chance that\n",
    "                        validation images may be used for training purposes as well.\n",
    "\n",
    "                        In that case, this isn't strictly a validation measure, instead of\n",
    "                        just a check to see what the network has learned.\n",
    "                        '''\n",
    "                        continue\n",
    "\n",
    "                    if pre_train_srgan:\n",
    "                        # Train only generator + vgg network\n",
    "\n",
    "                        # Use custom bypass_fit to bypass the check for same input and output batch size\n",
    "                        hist = bypass_fit(self.srgan_model_, [x_generator, x * 255], y_vgg_dummy,\n",
    "                                                     batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "                        sr_loss = hist.history['loss'][0]\n",
    "\n",
    "                        if save_loss:\n",
    "                            loss_history['generator_loss'].extend(hist.history['loss'])\n",
    "\n",
    "                        if prev_improvement == -1:\n",
    "                            prev_improvement = sr_loss\n",
    "\n",
    "                        improvement = (prev_improvement - sr_loss) / prev_improvement * 100\n",
    "                        prev_improvement = sr_loss\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "\n",
    "                        print(\"%d / %d | Improvement : %0.2f %% | %0.2f s/steps | \"\n",
    "                              \"GLoss: %0.2f\" % (iteration, num_images, improvement, t2 - t1, sr_loss))\n",
    "                    elif pre_train_discriminator:\n",
    "                        # Train only discriminator\n",
    "                        X_pred = self.generative_model_.predict(x_generator, self.batch_size)\n",
    "\n",
    "                        X = np.concatenate((X_pred, x * 255))\n",
    "\n",
    "                        # Using soft and noisy labels\n",
    "                        if np.random.uniform() > disc_train_flip:\n",
    "                            # give correct classifications\n",
    "                            y_gan = [0] * self.batch_size + [1] * self.batch_size\n",
    "                        else:\n",
    "                            # give wrong classifications (noisy labels)\n",
    "                            y_gan = [1] * self.batch_size + [0] * self.batch_size\n",
    "\n",
    "                        y_gan = np.asarray(y_gan, dtype=np.int).reshape(-1, 1)\n",
    "                        y_gan = to_categorical(y_gan, num_classes=2)\n",
    "                        y_gan = smooth_gan_labels(y_gan)\n",
    "\n",
    "                        hist = self.discriminative_model_.fit(X, y_gan, batch_size=self.batch_size,\n",
    "                                                              epochs=1, verbose=0)\n",
    "\n",
    "                        discriminator_loss = hist.history['loss'][-1]\n",
    "                        discriminator_acc = hist.history['acc'][-1]\n",
    "\n",
    "                        if save_loss:\n",
    "                            loss_history['discriminator_loss'].extend(hist.history['loss'])\n",
    "                            loss_history['discriminator_acc'].extend(hist.history['acc'])\n",
    "\n",
    "                        if prev_improvement == -1:\n",
    "                            prev_improvement = discriminator_loss\n",
    "\n",
    "                        improvement = (prev_improvement - discriminator_loss) / prev_improvement * 100\n",
    "                        prev_improvement = discriminator_loss\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "\n",
    "                        print(\"%d / %d | Improvement : %0.2f %% | %0.2f s/step | \"\n",
    "                            \"DLoss/Acc : %0.4f / %0.2f\" % (iteration, num_images,\n",
    "                                                            improvement, t2 - t1,\n",
    "                                                            discriminator_loss, discriminator_acc))\n",
    "\n",
    "                    else:\n",
    "                        # Train only discriminator, disable training of srgan\n",
    "                        self.discriminative_network.set_trainable(self.srgan_model_, value=True)\n",
    "                        self.generative_network.set_trainable(self.srgan_model_, value=False)\n",
    "\n",
    "                        # Use custom bypass_fit to bypass the check for same input and output batch size\n",
    "                        # hist = bypass_fit(self.srgan_model_, [x_generator, x * 255, x_vgg],\n",
    "                        #                          [y_gan, y_vgg_dummy],\n",
    "                        #                          batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "                        X_pred = self.generative_model_.predict(x_generator, self.batch_size)\n",
    "\n",
    "                        X = np.concatenate((X_pred, x * 255))\n",
    "\n",
    "                        # Using soft and noisy labels\n",
    "                        if np.random.uniform() > disc_train_flip:\n",
    "                            # give correct classifications\n",
    "                            y_gan = [0] * self.batch_size + [1] * self.batch_size\n",
    "                        else:\n",
    "                            # give wrong classifications (noisy labels)\n",
    "                            y_gan = [1] * self.batch_size + [0] * self.batch_size\n",
    "\n",
    "                        y_gan = np.asarray(y_gan, dtype=np.int).reshape(-1, 1)\n",
    "                        y_gan = to_categorical(y_gan, num_classes=2)\n",
    "                        y_gan = smooth_gan_labels(y_gan)\n",
    "\n",
    "                        hist1 = self.discriminative_model_.fit(X, y_gan, verbose=0, batch_size=self.batch_size,\n",
    "                                                              epochs=1)\n",
    "\n",
    "                        discriminator_loss = hist1.history['loss'][-1]\n",
    "\n",
    "                        # Train only generator, disable training of discriminator\n",
    "                        self.discriminative_network.set_trainable(self.srgan_model_, value=False)\n",
    "                        self.generative_network.set_trainable(self.srgan_model_, value=True)\n",
    "\n",
    "                        # Using soft labels\n",
    "                        y_model = [1] * self.batch_size\n",
    "                        y_model = np.asarray(y_model, dtype=np.int).reshape(-1, 1)\n",
    "                        y_model = to_categorical(y_model, num_classes=2)\n",
    "                        y_model = smooth_gan_labels(y_model)\n",
    "\n",
    "                        # Use custom bypass_fit to bypass the check for same input and output batch size\n",
    "                        hist2 = bypass_fit(self.srgan_model_, [x_generator, x, x_vgg], [y_model, y_vgg_dummy],\n",
    "                                           batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "                        generative_loss = hist2.history['loss'][0]\n",
    "\n",
    "                        if save_loss:\n",
    "                            loss_history['discriminator_loss'].extend(hist1.history['loss'])\n",
    "                            loss_history['discriminator_acc'].extend(hist1.history['acc'])\n",
    "                            loss_history['generator_loss'].extend(hist2.history['loss'])\n",
    "\n",
    "                        if prev_improvement == -1:\n",
    "                            prev_improvement = discriminator_loss\n",
    "\n",
    "                        improvement = (prev_improvement - discriminator_loss) / prev_improvement * 100\n",
    "                        prev_improvement = discriminator_loss\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "                        print(\"%d / %d | Improvement : %0.2f %% | %0.2f s/step | DLoss : %0.3f | GLoss : %0.3f\" %\n",
    "                              (iteration, num_images, improvement, t2 - t1, discriminator_loss, generative_loss))\n",
    "\n",
    "                    if iteration % 1000 == 0 and iteration != 0:\n",
    "                        print(\"Saving model weights.\")\n",
    "                        # Save predictive (SR network) weights\n",
    "                        self._save_model_weights(pre_train_srgan, pre_train_discriminator)\n",
    "                        self._save_loss_history(loss_history, pre_train_srgan, pre_train_discriminator, save_loss)\n",
    "\n",
    "                    if iteration >= num_images:\n",
    "                        break\n",
    "\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"Keyboard interrupt detected. Stopping early.\")\n",
    "                    early_stop = True\n",
    "                    break\n",
    "\n",
    "            iteration = 0\n",
    "\n",
    "            if early_stop:\n",
    "                break\n",
    "\n",
    "        print(\"Finished training SRGAN network. Saving model weights.\")\n",
    "        # Save predictive (SR network) weights\n",
    "        self._save_model_weights(pre_train_srgan, pre_train_discriminator)\n",
    "        self._save_loss_history(loss_history, pre_train_srgan, pre_train_discriminator, save_loss)\n",
    "\n",
    "    def _save_model_weights(self, pre_train_srgan, pre_train_discriminator):\n",
    "        if not pre_train_discriminator:\n",
    "            self.generative_model_.save_weights(self.generative_network.sr_weights_path, overwrite=True)\n",
    "\n",
    "        if not pre_train_srgan:\n",
    "            # Save GAN (discriminative network) weights\n",
    "            self.discriminative_network.save_gan_weights(self.discriminative_model_)\n",
    "\n",
    "    def _save_loss_history(self, loss_history, pre_train_srgan, pre_train_discriminator, save_loss):\n",
    "        if save_loss:\n",
    "            print(\"Saving loss history\")\n",
    "\n",
    "            if pre_train_srgan:\n",
    "                with open('pretrain losses - srgan.json', 'w') as f:\n",
    "                    json.dump(loss_history, f)\n",
    "            elif pre_train_discriminator:\n",
    "                with open('pretrain losses - discriminator.json', 'w') as f:\n",
    "                    json.dump(loss_history, f)\n",
    "            else:\n",
    "                with open('fulltrain losses.json', 'w') as f:\n",
    "                    json.dump(loss_history, f)\n",
    "\n",
    "            print(\"Saved loss history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to MS COCO dataset\n",
    "coco_path = r\"tests/coco\"\n",
    "\n",
    "\n",
    "#Base Network manager for the SRGAN model\n",
    "\n",
    "#Width / Height = 32 to reduce the memory requirement for the discriminator.\n",
    "\n",
    "#Batch size = 1 is slower, but uses the least amount of gpu memory, and also acts as\n",
    "#Instance Normalization (batch norm with 1 input image) which speeds up training slightly.\n",
    "\n",
    "srgan_network = SRGANNetwork(img_width=32, img_height=32, batch_size=1)\n",
    "srgan_network.build_srgan_model()\n",
    "srgan_network.srgan_model_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain the SRGAN network\n",
    "srgan_network.pre_train_srgan(coco_path, num_images=80000, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"pretrain losses - srgan.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Pretrain loss: Loaded SRGAN JSON.\")\n",
    "\n",
    "# plot the generator loss values\n",
    "print(\"Generator loss\")\n",
    "plt.plot(data['generator_loss'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean gan loss :\", np.mean(data['generator_loss']))\n",
    "print(\"Std gan loss : \", np.std(data['generator_loss']))\n",
    "print(\"Min gan loss : \", np.min(data['generator_loss']))\n",
    "\n",
    "# plot the PSNR loss values\n",
    "print(\"PSNR loss\")\n",
    "plt.plot(data['val_psnr'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean psnr loss :\", np.mean(data['val_psnr']))\n",
    "print(\"Std psnr loss : \", np.std(data['val_psnr']))\n",
    "print(\"Min psnr loss : \", np.min(data['val_psnr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrain the discriminator network\n",
    "srgan_network.pre_train_discriminator(coco_path, num_images=40000, epochs=1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"pretrain losses - discriminator.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Pretrain loss: Loaded discriminator JSON\")\n",
    "\n",
    "# plot the discriminator loss values\n",
    "print(\"Discriminator loss\")\n",
    "plt.plot(data['discriminator_loss'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean discriminator loss :\", np.mean(data['discriminator_loss']))\n",
    "print(\"Std discriminator loss : \", np.std(data['discriminator_loss']))\n",
    "print(\"Min discriminator loss : \", np.min(data['discriminator_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\stable\\lib\\site-packages\\ipykernel_launcher.py:105: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_generator (InputLayer)        (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv1 (Conv2D)           (None, 32, 32, 64)   4864        x_generator[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_1 (BatchNormalization (None, 32, 32, 64)   256         sr_res_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_lr1 (LeakyReLU)          (None, 32, 32, 64)   0           sr_res_bn_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv2 (Conv2D)           (None, 32, 32, 64)   102464      sr_res_lr1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_2 (BatchNormalization (None, 32, 32, 64)   256         sr_res_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_lr2 (LeakyReLU)          (None, 32, 32, 64)   0           sr_res_bn_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_1_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_lr2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_1_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_1_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_1_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_1_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_1_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_1_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_1_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_1_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_1 (Add)            (None, 32, 32, 64)   0           sr_res_bn_1_2[0][0]              \n",
      "                                                                 sr_res_lr2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_2_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_merge_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_2_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_2_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_2_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_2_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_2_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_2_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_2_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_2_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_2 (Add)            (None, 32, 32, 64)   0           sr_res_bn_2_2[0][0]              \n",
      "                                                                 sr_res_merge_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_3_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_merge_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_3_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_3_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_3_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_3_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_3_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_3_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_3_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_3_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_3 (Add)            (None, 32, 32, 64)   0           sr_res_bn_3_2[0][0]              \n",
      "                                                                 sr_res_merge_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_4_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_merge_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_4_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_4_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_4_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_4_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_4_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_4_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_4_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_4_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_4 (Add)            (None, 32, 32, 64)   0           sr_res_bn_4_2[0][0]              \n",
      "                                                                 sr_res_merge_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_5_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_merge_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_5_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_5_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_5_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_5_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_5_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_5_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_5_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_5_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_5 (Add)            (None, 32, 32, 64)   0           sr_res_bn_5_2[0][0]              \n",
      "                                                                 sr_res_merge_4[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_6_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_merge_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_6_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_6_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_6_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_6_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_6_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_6_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_6_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_6_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_6 (Add)            (None, 32, 32, 64)   0           sr_res_bn_6_2[0][0]              \n",
      "                                                                 sr_res_merge_5[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_7_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_merge_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_7_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_7_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_7_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_7_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_7_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_7_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_7_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_7_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_7 (Add)            (None, 32, 32, 64)   0           sr_res_bn_7_2[0][0]              \n",
      "                                                                 sr_res_merge_6[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_8_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_merge_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_8_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_8_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_8_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_8_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_8_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_8_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_8_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_8_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_8 (Add)            (None, 32, 32, 64)   0           sr_res_bn_8_2[0][0]              \n",
      "                                                                 sr_res_merge_7[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_9_1 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_merge_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_9_1 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_9_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_9_1 (LeakyReL (None, 32, 32, 64)   0           sr_res_bn_9_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_9_2 (Conv2D)        (None, 32, 32, 64)   36928       sr_res_activation_9_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_9_2 (BatchNormalizati (None, 32, 32, 64)   256         sr_res_conv_9_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_9 (Add)            (None, 32, 32, 64)   0           sr_res_bn_9_2[0][0]              \n",
      "                                                                 sr_res_merge_8[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_10_1 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_merge_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_10_1 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_10_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_10_1 (LeakyRe (None, 32, 32, 64)   0           sr_res_bn_10_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_10_2 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_activation_10_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_10_2 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_10_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_10 (Add)           (None, 32, 32, 64)   0           sr_res_bn_10_2[0][0]             \n",
      "                                                                 sr_res_merge_9[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_11_1 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_merge_10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_11_1 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_11_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_11_1 (LeakyRe (None, 32, 32, 64)   0           sr_res_bn_11_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_11_2 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_activation_11_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_11_2 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_11_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_11 (Add)           (None, 32, 32, 64)   0           sr_res_bn_11_2[0][0]             \n",
      "                                                                 sr_res_merge_10[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_12_1 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_merge_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_12_1 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_12_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_12_1 (LeakyRe (None, 32, 32, 64)   0           sr_res_bn_12_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_12_2 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_activation_12_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_12_2 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_12_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_12 (Add)           (None, 32, 32, 64)   0           sr_res_bn_12_2[0][0]             \n",
      "                                                                 sr_res_merge_11[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_13_1 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_merge_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_13_1 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_13_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_13_1 (LeakyRe (None, 32, 32, 64)   0           sr_res_bn_13_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_13_2 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_activation_13_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_13_2 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_13_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_13 (Add)           (None, 32, 32, 64)   0           sr_res_bn_13_2[0][0]             \n",
      "                                                                 sr_res_merge_12[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_14_1 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_merge_13[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_14_1 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_14_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_14_1 (LeakyRe (None, 32, 32, 64)   0           sr_res_bn_14_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_14_2 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_activation_14_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_14_2 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_14_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_14 (Add)           (None, 32, 32, 64)   0           sr_res_bn_14_2[0][0]             \n",
      "                                                                 sr_res_merge_13[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_15_1 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_merge_14[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_15_1 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_15_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_activation_15_1 (LeakyRe (None, 32, 32, 64)   0           sr_res_bn_15_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_15_2 (Conv2D)       (None, 32, 32, 64)   36928       sr_res_activation_15_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_bn_15_2 (BatchNormalizat (None, 32, 32, 64)   256         sr_res_conv_15_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_merge_15 (Add)           (None, 32, 32, 64)   0           sr_res_bn_15_2[0][0]             \n",
      "                                                                 sr_res_merge_14[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_upconv1_1 (Conv2D)       (None, 32, 32, 128)  73856       sr_res_merge_15[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_up_lr_1_1_1 (LeakyReLU)  (None, 32, 32, 128)  0           sr_res_upconv1_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_upscale_1 (UpSampling2D) (None, 64, 64, 128)  0           sr_res_up_lr_1_1_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_filter1_1 (Conv2D)       (None, 64, 64, 128)  147584      sr_res_upscale_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_up_lr_1_1_2 (LeakyReLU)  (None, 64, 64, 128)  0           sr_res_filter1_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_upconv1_2 (Conv2D)       (None, 64, 64, 128)  147584      sr_res_up_lr_1_1_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_up_lr_2_1_1 (LeakyReLU)  (None, 64, 64, 128)  0           sr_res_upconv1_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_upscale_2 (UpSampling2D) (None, 128, 128, 128 0           sr_res_up_lr_2_1_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_filter1_2 (Conv2D)       (None, 128, 128, 128 147584      sr_res_upscale_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_up_lr_2_1_2 (LeakyReLU)  (None, 128, 128, 128 0           sr_res_filter1_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_final (Conv2D)      (None, 128, 128, 3)  9603        sr_res_up_lr_2_1_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sr_res_conv_denorm (Denormalize (None, 128, 128, 3)  0           sr_res_conv_final[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "x_vgg (InputLayer)              (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 3)  0           sr_res_conv_denorm[0][0]         \n",
      "                                                                 x_vgg[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "normalize_vgg (Normalize)       (None, 128, 128, 3)  0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv1_1 (Conv2D)            (None, 128, 128, 64) 1792        normalize_vgg[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv1_2 (Conv2D)            (None, 128, 128, 64) 36928       vgg_conv1_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_maxpool1 (MaxPooling2D)     (None, 64, 64, 64)   0           vgg_conv1_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv2_1 (Conv2D)            (None, 64, 64, 128)  73856       vgg_maxpool1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv2_2 (Conv2D)            (None, 64, 64, 128)  147584      vgg_conv2_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_maxpool2 (MaxPooling2D)     (None, 32, 32, 128)  0           vgg_conv2_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv3_1 (Conv2D)            (None, 32, 32, 256)  295168      vgg_maxpool2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv3_2 (Conv2D)            (None, 32, 32, 256)  590080      vgg_conv3_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv3_3 (Conv2D)            (None, 32, 32, 256)  590080      vgg_conv3_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_maxpool3 (MaxPooling2D)     (None, 16, 16, 256)  0           vgg_conv3_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv4_1 (Conv2D)            (None, 16, 16, 512)  1180160     vgg_maxpool3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv4_2 (Conv2D)            (None, 16, 16, 512)  2359808     vgg_conv4_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv4_3 (Conv2D)            (None, 16, 16, 512)  2359808     vgg_conv4_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_maxpool4 (MaxPooling2D)     (None, 8, 8, 512)    0           vgg_conv4_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv5_1 (Conv2D)            (None, 8, 8, 512)    2359808     vgg_maxpool4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv5_2 (Conv2D)            (None, 8, 8, 512)    2359808     vgg_conv5_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "vgg_conv5_3 (Conv2D)            (None, 8, 8, 512)    2359808     vgg_conv5_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "model_8 (Model)                 (None, 2)            38250306    sr_res_conv_denorm[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "vgg_maxpool5 (MaxPooling2D)     (None, 4, 4, 512)    0           vgg_conv5_3[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 54,714,565\n",
      "Trainable params: 39,992,069\n",
      "Non-trainable params: 14,722,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Fully train the SRGAN with VGG loss and Discriminator loss\n",
    "srgan_network.train_full_model(coco_path, num_images=80000, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"fulltrain losses.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"Fulltrain loss: Loaded fulltrain JSON\")\n",
    "\n",
    "# plot the discriminator loss values\n",
    "print(\"Discriminator loss\")\n",
    "plt.plot(data['discriminator_loss'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean discriminator loss :\", np.mean(data['discriminator_loss']))\n",
    "print(\"Std discriminator loss : \", np.std(data['discriminator_loss']))\n",
    "print(\"Min discriminator loss : \", np.min(data['discriminator_loss']))\n",
    "\n",
    "\n",
    "# plot the generator loss values\n",
    "print(\"Generator loss\")\n",
    "plt.plot(data['generator_loss'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean generator loss :\", np.mean(data['generator_loss']))\n",
    "print(\"Std generator loss : \", np.std(data['generator_loss']))\n",
    "print(\"Min generator loss : \", np.min(data['generator_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "#import models\n",
    "from loss import PSNRLoss, psnr\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from imageio import imwrite as imsave\n",
    "from skimage.transform import resize as imresize\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "base_weights_path = \"weights/\"\n",
    "base_val_images_path = \"val_images/\"\n",
    "base_test_images = \"test_images/\"\n",
    "\n",
    "set5_path = \"tests/Set5\"\n",
    "set14_path = \"tests/Set14\"\n",
    "bsd100_path = \"tests/bsd100\"\n",
    "\n",
    "if not os.path.exists(base_weights_path):\n",
    "    os.makedirs(base_weights_path)\n",
    "\n",
    "if not os.path.exists(base_val_images_path):\n",
    "    os.makedirs(base_val_images_path)\n",
    "\n",
    "if not os.path.exists(base_test_images):\n",
    "    os.makedirs(base_test_images)\n",
    "\n",
    "def test_set5(model : Model, img_width=32, img_height=32, batch_size=1):\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    large_img_width = img_width * 4\n",
    "    large_img_height = img_height * 4\n",
    "\n",
    "    iteration = 0\n",
    "    total_psnr = 0.0\n",
    "\n",
    "    print(\"Testing model on Set 5 Validation images\")\n",
    "    total_psnr = _test_loop(set5_path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width,\n",
    "                            model, total_psnr, \"Set5\", 5)\n",
    "\n",
    "    print(\"Average PSNR of Set5 validation images : \", total_psnr / 5)\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_set14(model : Model, img_width=32, img_height=32, batch_size=1):\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    large_img_width = img_width * 4\n",
    "    large_img_height = img_height * 4\n",
    "\n",
    "    iteration = 0\n",
    "    total_psnr = 0.0\n",
    "\n",
    "    print(\"Testing model on Set 14 Validation images\")\n",
    "    total_psnr = _test_loop(set14_path, batch_size, datagen, img_height, img_width, iteration, large_img_height,\n",
    "                            large_img_width, model, total_psnr, \"Set14\", 14)\n",
    "\n",
    "    print(\"Average PSNR of Set5 validation images : \", total_psnr / 14)\n",
    "    print()\n",
    "\n",
    "def test_bsd100(model : Model, img_width=32, img_height=32, batch_size=1):\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    large_img_width = img_width * 4\n",
    "    large_img_height = img_height * 4\n",
    "\n",
    "    iteration = 0\n",
    "    total_psnr = 0.0\n",
    "\n",
    "    print(\"Testing model on BSD 100 Validation images\")\n",
    "    total_psnr = _test_loop(bsd100_path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width,\n",
    "                            model, total_psnr, \"bsd100\", 100)\n",
    "\n",
    "    print(\"Average PSNR of BSD100 validation images : \", total_psnr / 100)\n",
    "    print()\n",
    "\n",
    "\n",
    "def _test_loop(path, batch_size, datagen, img_height, img_width, iteration, large_img_height, large_img_width, model,\n",
    "               total_psnr, prefix, num_images):\n",
    "    for x in datagen.flow_from_directory(path, class_mode=None, batch_size=batch_size,\n",
    "                                         target_size=(large_img_width, large_img_height)):\n",
    "        t1 = time.time()\n",
    "\n",
    "        # resize images\n",
    "        x_temp = x.copy()\n",
    "        x_temp = x_temp.transpose((0, 2, 3, 1))\n",
    "\n",
    "        x_generator = np.empty((batch_size, img_width, img_height, 3))\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            img = imresize(x_temp[j], (img_width, img_height))\n",
    "            x_generator[j, :, :, :] = img\n",
    "\n",
    "        x_generator = x_generator.transpose((0, 3, 1, 2))\n",
    "\n",
    "        output_image_batch = model.predict_on_batch(x_generator)\n",
    "\n",
    "        average_psnr = 0.0\n",
    "        for x_i in range(batch_size):\n",
    "            average_psnr += psnr(x[x_i], output_image_batch[x_i] / 255.)\n",
    "            total_psnr += average_psnr\n",
    "\n",
    "        average_psnr /= batch_size\n",
    "\n",
    "        iteration += batch_size\n",
    "        t2 = time.time()\n",
    "\n",
    "        print(\"Time required : %0.2f. Average validation PSNR over %d samples = %0.2f\" %\n",
    "              (t2 - t1, batch_size, average_psnr))\n",
    "\n",
    "        for x_i in range(batch_size):\n",
    "            real_path = base_test_images + prefix + \"_iteration_%d_num_%d_real_.png\" % (iteration, x_i + 1)\n",
    "            generated_path = base_test_images + prefix + \"_iteration_%d_num_%d_generated.png\" % (iteration, x_i + 1)\n",
    "\n",
    "            val_x = x[x_i].copy() * 255.\n",
    "            val_x = val_x.transpose((1, 2, 0))\n",
    "            val_x = np.clip(val_x, 0, 255).astype('uint8')\n",
    "\n",
    "            output_image = output_image_batch[x_i]\n",
    "            output_image = output_image.transpose((1, 2, 0))\n",
    "            output_image = np.clip(output_image, 0, 255).astype('uint8')\n",
    "\n",
    "            imsave(real_path, val_x[:,:,0])\n",
    "            imsave(generated_path, output_image[:,:,0])\n",
    "\n",
    "        if iteration >= num_images:\n",
    "            break\n",
    "    return total_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRResNetTest:\n",
    "\n",
    "    def __init__(self, img_width=96, img_height=96, batch_size=16):\n",
    "        assert img_width >= 16, \"Minimum image width must be at least 16\"\n",
    "        assert img_height >= 16, \"Minimum image height must be at least 16\"\n",
    "\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = None # type: Model\n",
    "        self.weights_path = base_weights_path + \"sr_resnet_weights.h5\"\n",
    "\n",
    "    def build_model(self, load_weights=False) -> Model:\n",
    "        sr_resnet = GenerativeNetwork(self.img_width, self.img_height, self.batch_size)\n",
    "\n",
    "        ip = Input(shape=(self.img_width, self.img_height, 3), name='x_generator')\n",
    "        output = sr_resnet.create_sr_model(ip)\n",
    "\n",
    "        self.model = Model(ip, output)\n",
    "\n",
    "        optimizer = Adam(lr=1e-4)\n",
    "        self.model.compile(optimizer, loss='mse', metrics=[PSNRLoss])\n",
    "\n",
    "        if load_weights:\n",
    "            try:\n",
    "                self.model.load_weights(self.weights_path)\n",
    "                print(\"SR ResNet model weights loaded.\")\n",
    "            except Exception:\n",
    "                print(\"Weight for SR ResNet model not found or are incorrect size. Cannot load weights.\")\n",
    "\n",
    "                response = input(\"Continue without loading weights? 'y' or 'n' \")\n",
    "                if response == 'n':\n",
    "                    exit()\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, image_dir, num_images=50000, epochs=1):\n",
    "        datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        img_width = self.img_width * 4\n",
    "        img_height = self.img_height * 4\n",
    "\n",
    "        early_stop = False\n",
    "        iteration = 0\n",
    "        prev_improvement = -1\n",
    "\n",
    "        print(\"Training SR ResNet network\")\n",
    "        for i in range(epochs):\n",
    "            print()\n",
    "            print(\"Epoch : %d\" % (i + 1))\n",
    "\n",
    "            for x in datagen.flow_from_directory(image_dir, class_mode=None, batch_size=self.batch_size,\n",
    "                                                 target_size=(img_width, img_height)):\n",
    "\n",
    "                try:\n",
    "                    t1 = time.time()\n",
    "\n",
    "                    # resize images\n",
    "                    x_temp = x.copy()\n",
    "                    x_temp = x_temp.transpose((0, 2, 3, 1))\n",
    "\n",
    "                    x_generator = np.empty((self.batch_size, self.img_width, self.img_height, 3))\n",
    "\n",
    "                    for j in range(self.batch_size):\n",
    "                        img = gaussian_filter(x_temp[j], sigma=0.5)\n",
    "                        img = imresize(img, (self.img_width, self.img_height,3))\n",
    "                        x_generator[j, :, :, :] = img\n",
    "\n",
    "                    #x_generator = x_generator.transpose((0, 3, 1, 2))\n",
    "\n",
    "                    if iteration % 50 == 0 and iteration != 0 :\n",
    "                        print(\"Random Validation image..\")\n",
    "                        output_image_batch = self.model.predict_on_batch(x_generator)\n",
    "\n",
    "                        print(\"Pred Max / Min: %0.2f / %0.2f\" % (output_image_batch.max(),\n",
    "                                                                 output_image_batch.min()))\n",
    "\n",
    "                        average_psnr = 0.0\n",
    "                        for x_i in range(self.batch_size):\n",
    "                            average_psnr += psnr(x[x_i], output_image_batch[x_i] / 255.)\n",
    "\n",
    "                        average_psnr /= self.batch_size\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "\n",
    "                        print(\"Time required : %0.2f. Average validation PSNR over %d samples = %0.2f\" %\n",
    "                              (t2 - t1, self.batch_size, average_psnr))\n",
    "\n",
    "                        for x_i in range(self.batch_size):\n",
    "                            real_path = base_val_images_path + \"epoch_%d_iteration_%d_num_%d_real_.png\" % \\\n",
    "                                                               (i + 1, iteration, x_i + 1)\n",
    "\n",
    "                            generated_path = base_val_images_path + \\\n",
    "                                             \"epoch_%d_iteration_%d_num_%d_generated.png\" % (i + 1,\n",
    "                                                                                            iteration,\n",
    "                                                                                            x_i + 1)\n",
    "\n",
    "                            val_x = x[x_i].copy() * 255.\n",
    "                            val_x = val_x.transpose((1, 2, 0))\n",
    "                            val_x = np.clip(val_x, 0, 255).astype('uint8')\n",
    "\n",
    "                            output_image = output_image_batch[x_i]\n",
    "                            output_image = output_image.transpose((1, 2, 0))\n",
    "                            output_image = np.clip(output_image, 0, 255).astype('uint8')\n",
    "\n",
    "                            imsave(real_path, val_x[:,:,0])\n",
    "                            imsave(generated_path, output_image[:,:,0])\n",
    "\n",
    "                        '''\n",
    "                        Don't train of validation images for now.\n",
    "\n",
    "                        Note that if epochs > 1, there is a chance that\n",
    "                        validation images may be used for training purposes as well.\n",
    "\n",
    "                        In that case, this isn't strictly a validation measure, instead of\n",
    "                        just a check to see what the network has learned.\n",
    "                        '''\n",
    "                        continue\n",
    "\n",
    "                    hist = self.model.fit(x_generator, x * 255, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "                    psnr_loss_val = hist.history['PSNRLoss'][0]\n",
    "\n",
    "                    if prev_improvement == -1:\n",
    "                        prev_improvement = psnr_loss_val\n",
    "\n",
    "                    improvement = (prev_improvement - psnr_loss_val) / prev_improvement * 100\n",
    "                    prev_improvement = psnr_loss_val\n",
    "\n",
    "                    iteration += self.batch_size\n",
    "                    t2 = time.time()\n",
    "\n",
    "                    print(\"%d / %d | Improvement : %0.2f %% | Time required : %0.2f s/step | \"\n",
    "                          \"PSNR : %0.3f\" % (iteration, num_images, improvement, t2 - t1, psnr_loss_val))\n",
    "\n",
    "                    if iteration % 1000 == 0 and iteration != 0:\n",
    "                        print(\"Saving weights\")\n",
    "                        self.model.save_weights(self.weights_path, overwrite=True)\n",
    "\n",
    "                    if iteration >= num_images:\n",
    "                        break\n",
    "\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"Keyboard interrupt detected. Stopping early.\")\n",
    "                    early_stop = True\n",
    "                    break\n",
    "\n",
    "            iteration = 0\n",
    "\n",
    "            if early_stop:\n",
    "                break\n",
    "\n",
    "        print(\"Finished training SRGAN network. Saving model weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_path = \"tests/coco\"\n",
    "\n",
    "img_width = img_height = 64\n",
    "\n",
    "sr_resnet_test = SRResNetTest(img_width=img_width, img_height=img_height, batch_size=1)\n",
    "sr_resnet_test.build_model(load_weights=False)\n",
    "\n",
    "sr_resnet_test.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is option if we need to train model on the go and then do the testing\n",
    "#sr_resnet_test.train_model(coco_path, num_images=50000, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set5(sr_resnet_test.model, img_width=img_width, img_height=img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set14(sr_resnet_test.model, img_width=img_width, img_height=img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bsd100(sr_resnet_test.model, img_width=img_width, img_height=img_height)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
