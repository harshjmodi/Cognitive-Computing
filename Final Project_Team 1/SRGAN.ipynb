{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, BatchNormalization, LeakyReLU, Flatten, Dense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from keras_ops import fit as bypass_fit, smooth_gan_labels\n",
    "\n",
    "from layers import Normalize, Denormalize, SubPixelUpscaling\n",
    "from loss import AdversarialLossRegularizer, ContentVGGRegularizer, TVRegularizer, psnr, dummy_loss\n",
    "\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.misc import imresize, imsave\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "THEANO_WEIGHTS_PATH_NO_TOP = r'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels_notop.h5'\n",
    "TF_WEIGHTS_PATH_NO_TOP = r\"https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "if not os.path.exists(\"weights/\"):\n",
    "    os.makedirs(\"weights/\")\n",
    "\n",
    "if not os.path.exists(\"val_images/\"):\n",
    "    os.makedirs(\"val_images/\")\n",
    "\n",
    "if K.image_dim_ordering() == \"th\":\n",
    "    channel_axis = 1\n",
    "else:\n",
    "    channel_axis = -1\n",
    "\n",
    "class VGGNetwork:\n",
    "    '''\n",
    "    Helper class to load VGG and its weights to the FastNet model\n",
    "    '''\n",
    "\n",
    "    def __init__(self, img_width=384, img_height=384, vgg_weight=1.0):\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.vgg_weight = vgg_weight\n",
    "\n",
    "        self.vgg_layers = None\n",
    "\n",
    "    def append_vgg_network(self, x_in, true_X_input, pre_train=False):\n",
    "\n",
    "        # Append the initial inputs to the outputs of the SRResNet\n",
    "        x = merge([x_in, true_X_input], mode='concat', concat_axis=0)\n",
    "\n",
    "        # Normalize the inputs via custom VGG Normalization layer\n",
    "        x = Normalize(name=\"normalize_vgg\")(x)\n",
    "\n",
    "        # Begin adding the VGG layers\n",
    "        x = Convolution2D(64, 3, 3, activation='relu', name='vgg_conv1_1', border_mode='same')(x)\n",
    "\n",
    "        x = Convolution2D(64, 3, 3, activation='relu', name='vgg_conv1_2', border_mode='same')(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool1')(x)\n",
    "\n",
    "        x = Convolution2D(128, 3, 3, activation='relu', name='vgg_conv2_1', border_mode='same')(x)\n",
    "\n",
    "        if pre_train:\n",
    "            vgg_regularizer2 = ContentVGGRegularizer(weight=self.vgg_weight)\n",
    "            x = Convolution2D(128, 3, 3, activation='relu', name='vgg_conv2_2', border_mode='same',\n",
    "                              activity_regularizer=vgg_regularizer2)(x)\n",
    "        else:\n",
    "            x = Convolution2D(128, 3, 3, activation='relu', name='vgg_conv2_2', border_mode='same')(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool2')(x)\n",
    "\n",
    "        x = Convolution2D(256, 3, 3, activation='relu', name='vgg_conv3_1', border_mode='same')(x)\n",
    "        x = Convolution2D(256, 3, 3, activation='relu', name='vgg_conv3_2', border_mode='same')(x)\n",
    "\n",
    "        x = Convolution2D(256, 3, 3, activation='relu', name='vgg_conv3_3', border_mode='same')(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool3')(x)\n",
    "\n",
    "        x = Convolution2D(512, 3, 3, activation='relu', name='vgg_conv4_1', border_mode='same')(x)\n",
    "        x = Convolution2D(512, 3, 3, activation='relu', name='vgg_conv4_2', border_mode='same')(x)\n",
    "\n",
    "        x = Convolution2D(512, 3, 3, activation='relu', name='vgg_conv4_3', border_mode='same')(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool4')(x)\n",
    "\n",
    "        x = Convolution2D(512, 3, 3, activation='relu', name='vgg_conv5_1', border_mode='same')(x)\n",
    "        x = Convolution2D(512, 3, 3, activation='relu', name='vgg_conv5_2', border_mode='same')(x)\n",
    "\n",
    "        if not pre_train:\n",
    "            vgg_regularizer5 = ContentVGGRegularizer(weight=self.vgg_weight)\n",
    "            x = Convolution2D(512, 3, 3, activation='relu', name='vgg_conv5_3', border_mode='same',\n",
    "                          activity_regularizer=vgg_regularizer5)(x)\n",
    "        else:\n",
    "            x = Convolution2D(512, 3, 3, activation='relu', name='vgg_conv5_3', border_mode='same')(x)\n",
    "        x = MaxPooling2D(name='vgg_maxpool5')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def load_vgg_weight(self, model):\n",
    "        # Loading VGG 16 weights\n",
    "        if K.image_dim_ordering() == \"th\":\n",
    "            weights = get_file('vgg16_weights_th_dim_ordering_th_kernels_notop.h5', THEANO_WEIGHTS_PATH_NO_TOP,\n",
    "                                   cache_subdir='models')\n",
    "        else:\n",
    "            weights = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', TF_WEIGHTS_PATH_NO_TOP,\n",
    "                                   cache_subdir='models')\n",
    "        f = h5py.File(weights)\n",
    "\n",
    "        layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "        if self.vgg_layers is None:\n",
    "            self.vgg_layers = [layer for layer in model.layers\n",
    "                               if 'vgg_' in layer.name]\n",
    "\n",
    "        for i, layer in enumerate(self.vgg_layers):\n",
    "            g = f[layer_names[i]]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "\n",
    "        # Freeze all VGG layers\n",
    "        for layer in self.vgg_layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "class DiscriminatorNetwork:\n",
    "\n",
    "    def __init__(self, img_width=384, img_height=384, adversarial_loss_weight=1, small_model=False):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.adversarial_loss_weight = adversarial_loss_weight\n",
    "        self.small_model = small_model\n",
    "\n",
    "        self.k = 3\n",
    "        self.mode = 2\n",
    "        self.weights_path = \"weights/Discriminator weights.h5\"\n",
    "\n",
    "        self.gan_layers = None\n",
    "\n",
    "    def append_gan_network(self, true_X_input):\n",
    "\n",
    "        # Normalize the inputs via custom VGG Normalization layer\n",
    "        x = Normalize(type=\"gan\", value=127.5, name=\"gan_normalize\")(true_X_input)\n",
    "\n",
    "        x = Convolution2D(64, self.k, self.k, border_mode='same', name='gan_conv1_1')(x)\n",
    "        x = LeakyReLU(0.3, name=\"gan_lrelu1_1\")(x)\n",
    "\n",
    "        x = Convolution2D(64, self.k, self.k, border_mode='same', name='gan_conv1_2', subsample=(2, 2))(x)\n",
    "        x = LeakyReLU(0.3, name='gan_lrelu1_2')(x)\n",
    "        x = BatchNormalization(mode=self.mode, axis=channel_axis, name='gan_batchnorm1_1')(x)\n",
    "\n",
    "        filters = [128, 256] if self.small_model else [128, 256, 512]\n",
    "\n",
    "        for i, nb_filters in enumerate(filters):\n",
    "            for j in range(2):\n",
    "                subsample = (2, 2) if j == 1 else (1, 1)\n",
    "\n",
    "                x = Convolution2D(nb_filters, self.k, self.k, border_mode='same', subsample=subsample,\n",
    "                                  name='gan_conv%d_%d' % (i + 2, j + 1))(x)\n",
    "                x = LeakyReLU(0.3, name='gan_lrelu_%d_%d' % (i + 2, j + 1))(x)\n",
    "                x = BatchNormalization(mode=self.mode, axis=channel_axis, name='gan_batchnorm%d_%d' % (i + 2, j + 1))(x)\n",
    "\n",
    "        x = Flatten(name='gan_flatten')(x)\n",
    "\n",
    "        output_dim = 128 if self.small_model else 1024\n",
    "\n",
    "        x = Dense(output_dim, name='gan_dense1')(x)\n",
    "        x = LeakyReLU(0.3, name='gan_lrelu5')(x)\n",
    "\n",
    "        gan_regulrizer = AdversarialLossRegularizer(weight=self.adversarial_loss_weight)\n",
    "        x = Dense(2, activation=\"softmax\", activity_regularizer=gan_regulrizer, name='gan_output')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_trainable(self, model, value=True):\n",
    "        if self.gan_layers is None:\n",
    "            disc_model = [layer for layer in model.layers\n",
    "                          if 'model' in layer.name][0] # Only disc model is an inner model\n",
    "\n",
    "            self.gan_layers = [layer for layer in disc_model.layers\n",
    "                               if 'gan_' in layer.name]\n",
    "\n",
    "        for layer in self.gan_layers:\n",
    "            layer.trainable = value\n",
    "\n",
    "    def load_gan_weights(self, model):\n",
    "        f = h5py.File(self.weights_path)\n",
    "\n",
    "        layer_names = [name for name in f.attrs['layer_names']]\n",
    "        layer_names = layer_names[1:] # First is an input layer. Not needed.\n",
    "\n",
    "        if self.gan_layers is None:\n",
    "            self.gan_layers = [layer for layer in model.layers\n",
    "                                if 'gan_' in layer.name]\n",
    "\n",
    "        for i, layer in enumerate(self.gan_layers):\n",
    "            g = f[layer_names[i]]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "\n",
    "        print(\"GAN Model weights loaded.\")\n",
    "        return model\n",
    "\n",
    "    def save_gan_weights(self, model):\n",
    "        print('GAN Weights are being saved.')\n",
    "        model.save_weights(self.weights_path, overwrite=True)\n",
    "        print('GAN Weights saved.')\n",
    "\n",
    "\n",
    "class GenerativeNetwork:\n",
    "\n",
    "    def __init__(self, img_width=96, img_height=96, batch_size=16, nb_upscales=2, small_model=False,\n",
    "                 content_weight=1, tv_weight=2e5, gen_channels=64):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "        self.small_model = small_model\n",
    "        self.nb_scales = nb_upscales\n",
    "\n",
    "        self.content_weight = content_weight\n",
    "        self.tv_weight = tv_weight\n",
    "\n",
    "        self.filters = gen_channels\n",
    "        self.mode = 2\n",
    "        self.init = 'glorot_uniform'\n",
    "\n",
    "        self.sr_res_layers = None\n",
    "        self.sr_weights_path = \"weights/SRGAN.h5\"\n",
    "\n",
    "        self.output_func = None\n",
    "\n",
    "    def create_sr_model(self, ip):\n",
    "\n",
    "        x = Convolution2D(self.filters, 5, 5, activation='linear', border_mode='same', name='sr_res_conv1',\n",
    "                          init=self.init)(ip)\n",
    "        x = BatchNormalization(axis=channel_axis, mode=self.mode, name='sr_res_bn_1')(x)\n",
    "        x = LeakyReLU(alpha=0.25, name='sr_res_lr1')(x)\n",
    "\n",
    "        # x = Convolution2D(self.filters, 5, 5, activation='linear', border_mode='same', name='sr_res_conv2')(x)\n",
    "        # x = BatchNormalization(axis=channel_axis, mode=self.mode, name='sr_res_bn_2')(x)\n",
    "        # x = LeakyReLU(alpha=0.25, name='sr_res_lr2')(x)\n",
    "\n",
    "        nb_residual = 5 if self.small_model else 15\n",
    "\n",
    "        for i in range(nb_residual):\n",
    "            x = self._residual_block(x, i + 1)\n",
    "\n",
    "        for scale in range(self.nb_scales):\n",
    "            x = self._upscale_block(x, scale + 1)\n",
    "\n",
    "        scale = 2 ** self.nb_scales\n",
    "        tv_regularizer = TVRegularizer(img_width=self.img_width * scale, img_height=self.img_height * scale,\n",
    "                                       weight=self.tv_weight) #self.tv_weight)\n",
    "\n",
    "        x = Convolution2D(3, 5, 5, activation='tanh', border_mode='same', activity_regularizer=tv_regularizer,\n",
    "                          init=self.init, name='sr_res_conv_final')(x)\n",
    "\n",
    "        x = Denormalize(name='sr_res_conv_denorm')(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _residual_block(self, ip, id):\n",
    "        init = ip\n",
    "\n",
    "        x = Convolution2D(self.filters, 3, 3, activation='linear', border_mode='same', name='sr_res_conv_' + str(id) + '_1',\n",
    "                          init=self.init)(ip)\n",
    "        x = BatchNormalization(axis=channel_axis, mode=self.mode, name='sr_res_bn_' + str(id) + '_1')(x)\n",
    "        x = LeakyReLU(alpha=0.25, name=\"sr_res_activation_\" + str(id) + \"_1\")(x)\n",
    "\n",
    "        x = Convolution2D(self.filters, 3, 3, activation='linear', border_mode='same', name='sr_res_conv_' + str(id) + '_2',\n",
    "                          init=self.init)(x)\n",
    "        x = BatchNormalization(axis=channel_axis, mode=self.mode, name='sr_res_bn_' + str(id) + '_2')(x)\n",
    "\n",
    "        m = merge([x, init], mode='sum', name=\"sr_res_merge_\" + str(id))\n",
    "\n",
    "        return m\n",
    "\n",
    "    def _upscale_block(self, ip, id):\n",
    "        '''\n",
    "        As per suggestion from http://distill.pub/2016/deconv-checkerboard/, I am swapping out\n",
    "        SubPixelConvolution to simple Nearest Neighbour Upsampling\n",
    "        '''\n",
    "        init = ip\n",
    "\n",
    "        x = Convolution2D(128, 3, 3, activation=\"linear\", border_mode='same', name='sr_res_upconv1_%d' % id,\n",
    "                          init=self.init)(init)\n",
    "        x = LeakyReLU(alpha=0.25, name='sr_res_up_lr_%d_1_1' % id)(x)\n",
    "        x = UpSampling2D(name='sr_res_upscale_%d' % id)(x)\n",
    "        #x = SubPixelUpscaling(r=2, channels=32)(x)\n",
    "        x = Convolution2D(128, 3, 3, activation=\"linear\", border_mode='same', name='sr_res_filter1_%d' % id,\n",
    "                          init=self.init)(x)\n",
    "        x = LeakyReLU(alpha=0.3, name='sr_res_up_lr_%d_1_2' % id)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def set_trainable(self, model, value=True):\n",
    "        if self.sr_res_layers is None:\n",
    "            self.sr_res_layers = [layer for layer in model.layers\n",
    "                                    if 'sr_res_' in layer.name]\n",
    "\n",
    "        for layer in self.sr_res_layers:\n",
    "            layer.trainable = value\n",
    "\n",
    "    def get_generator_output(self, input_img, srgan_model):\n",
    "        if self.output_func is None:\n",
    "            gen_output_layer = [layer for layer in srgan_model.layers\n",
    "                                if layer.name == \"sr_res_conv_denorm\"][0]\n",
    "            self.output_func = K.function([srgan_model.layers[0].input],\n",
    "                                          [gen_output_layer.output])\n",
    "\n",
    "        return self.output_func([input_img])\n",
    "\n",
    "\n",
    "class SRGANNetwork:\n",
    "\n",
    "    def __init__(self, img_width=96, img_height=96, batch_size=16, nb_scales=2):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_scales = nb_scales\n",
    "\n",
    "        self.discriminative_network = None # type: DiscriminatorNetwork\n",
    "        self.generative_network = None # type: GenerativeNetwork\n",
    "        self.vgg_network = None # type: VGGNetwork\n",
    "\n",
    "        self.srgan_model_ = None # type: Model\n",
    "        self.generative_model_ = None # type: Model\n",
    "        self.discriminative_model_ = None #type: Model\n",
    "\n",
    "    def build_srgan_pretrain_model(self, use_small_srgan=False):\n",
    "        large_width = self.img_width * 4\n",
    "        large_height = self.img_height * 4\n",
    "\n",
    "        self.generative_network = GenerativeNetwork(self.img_width, self.img_height, self.batch_size, self.nb_scales,\n",
    "                                                    use_small_srgan)\n",
    "        self.vgg_network = VGGNetwork(large_width, large_height)\n",
    "\n",
    "        ip = Input(shape=(3, self.img_width, self.img_height), name='x_generator')\n",
    "        ip_vgg = Input(shape=(3, large_width, large_height), name='x_vgg')  # Actual X images\n",
    "\n",
    "        sr_output = self.generative_network.create_sr_model(ip)\n",
    "        self.generative_model_ = Model(ip, sr_output)\n",
    "\n",
    "        vgg_output = self.vgg_network.append_vgg_network(sr_output, ip_vgg, pre_train=True)\n",
    "\n",
    "        self.srgan_model_ = Model(input=[ip, ip_vgg],\n",
    "                                  output=vgg_output)\n",
    "\n",
    "        self.vgg_network.load_vgg_weight(self.srgan_model_)\n",
    "\n",
    "        srgan_optimizer = Adam(lr=1e-4)\n",
    "        generator_optimizer = Adam(lr=1e-4)\n",
    "\n",
    "        self.generative_model_.compile(generator_optimizer, dummy_loss)\n",
    "        self.srgan_model_.compile(srgan_optimizer, dummy_loss)\n",
    "\n",
    "        return self.srgan_model_\n",
    "\n",
    "\n",
    "    def build_discriminator_pretrain_model(self, use_small_srgan=False, use_small_discriminator=False):\n",
    "        large_width = self.img_width * 4\n",
    "        large_height = self.img_height * 4\n",
    "\n",
    "        self.generative_network = GenerativeNetwork(self.img_width, self.img_height, self.batch_size, self.nb_scales,\n",
    "                                                    use_small_srgan)\n",
    "        self.discriminative_network = DiscriminatorNetwork(large_width, large_height,\n",
    "                                                           small_model=use_small_discriminator)\n",
    "\n",
    "        ip = Input(shape=(3, self.img_width, self.img_height), name='x_generator')\n",
    "        ip_gan = Input(shape=(3, large_width, large_height), name='x_discriminator')  # Actual X images\n",
    "\n",
    "        sr_output = self.generative_network.create_sr_model(ip)\n",
    "        self.generative_model_ = Model(ip, sr_output)\n",
    "        #self.generative_network.set_trainable(self.generative_model_, value=False)\n",
    "\n",
    "        gan_output = self.discriminative_network.append_gan_network(ip_gan)\n",
    "        self.discriminative_model_ = Model(ip_gan, gan_output)\n",
    "\n",
    "        generator_out = self.generative_model_(ip)\n",
    "        gan_output = self.discriminative_model_(generator_out)\n",
    "\n",
    "        self.srgan_model_ = Model(input=ip, output=gan_output)\n",
    "\n",
    "        srgan_optimizer = Adam(lr=1e-4)\n",
    "        generator_optimizer = Adam(lr=1e-4)\n",
    "        discriminator_optimizer = Adam(lr=1e-4)\n",
    "\n",
    "        self.generative_model_.compile(generator_optimizer, loss='mse')\n",
    "        self.discriminative_model_.compile(discriminator_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "        self.srgan_model_.compile(srgan_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "        return self.discriminative_model_\n",
    "\n",
    "\n",
    "    def build_srgan_model(self, use_small_srgan=False, use_small_discriminator=False):\n",
    "        large_width = self.img_width * 4\n",
    "        large_height = self.img_height * 4\n",
    "\n",
    "        self.generative_network = GenerativeNetwork(self.img_width, self.img_height, self.batch_size, nb_upscales=self.nb_scales,\n",
    "                                                    small_model=use_small_srgan)\n",
    "        self.discriminative_network = DiscriminatorNetwork(large_width, large_height,\n",
    "                                                           small_model=use_small_discriminator)\n",
    "        self.vgg_network = VGGNetwork(large_width, large_height)\n",
    "\n",
    "        ip = Input(shape=(3, self.img_width, self.img_height), name='x_generator')\n",
    "        ip_gan = Input(shape=(3, large_width, large_height), name='x_discriminator') # Actual X images\n",
    "        ip_vgg = Input(shape=(3, large_width, large_height), name='x_vgg') # Actual X images\n",
    "\n",
    "        sr_output = self.generative_network.create_sr_model(ip)\n",
    "        self.generative_model_ = Model(ip, sr_output)\n",
    "\n",
    "        gan_output = self.discriminative_network.append_gan_network(ip_gan)\n",
    "        self.discriminative_model_ = Model(ip_gan, gan_output)\n",
    "\n",
    "        gan_output = self.discriminative_model_(self.generative_model_.output)\n",
    "        vgg_output = self.vgg_network.append_vgg_network(self.generative_model_.output, ip_vgg)\n",
    "\n",
    "        self.srgan_model_ = Model(input=[ip, ip_gan, ip_vgg], output=[gan_output, vgg_output])\n",
    "\n",
    "        self.vgg_network.load_vgg_weight(self.srgan_model_)\n",
    "\n",
    "        srgan_optimizer = Adam(lr=1e-4)\n",
    "        generator_optimizer = Adam(lr=1e-4)\n",
    "        discriminator_optimizer = Adam(lr=1e-4)\n",
    "\n",
    "        self.generative_model_.compile(generator_optimizer, dummy_loss)\n",
    "        self.discriminative_model_.compile(discriminator_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "        self.srgan_model_.compile(srgan_optimizer, dummy_loss)\n",
    "\n",
    "        return self.srgan_model_\n",
    "\n",
    "\n",
    "    def pre_train_srgan(self, image_dir, nb_images=50000, nb_epochs=1, use_small_srgan=False):\n",
    "        self.build_srgan_pretrain_model(use_small_srgan=use_small_srgan)\n",
    "\n",
    "        self._train_model(image_dir, nb_images=nb_images, nb_epochs=nb_epochs, pre_train_srgan=True,\n",
    "                          load_generative_weights=True)\n",
    "\n",
    "    def pre_train_discriminator(self, image_dir, nb_images=50000, nb_epochs=1, batch_size=128,\n",
    "                                use_small_discriminator=False):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.build_discriminator_pretrain_model(use_small_discriminator)\n",
    "\n",
    "        self._train_model(image_dir, nb_images, nb_epochs, pre_train_discriminator=True,\n",
    "                          load_generative_weights=True)\n",
    "\n",
    "    def train_full_model(self, image_dir, nb_images=50000, nb_epochs=10, use_small_srgan=False,\n",
    "                         use_small_discriminator=False):\n",
    "\n",
    "        self.build_srgan_model(use_small_srgan, use_small_discriminator)\n",
    "\n",
    "        self._train_model(image_dir, nb_images, nb_epochs, load_generative_weights=True, load_discriminator_weights=True)\n",
    "\n",
    "    def _train_model(self, image_dir, nb_images=80000, nb_epochs=10, pre_train_srgan=False,\n",
    "                     pre_train_discriminator=False, load_generative_weights=False, load_discriminator_weights=False,\n",
    "                     save_loss=True, disc_train_flip=0.1):\n",
    "\n",
    "        assert self.img_width >= 16, \"Minimum image width must be at least 16\"\n",
    "        assert self.img_height >= 16, \"Minimum image height must be at least 16\"\n",
    "\n",
    "        if load_generative_weights:\n",
    "            try:\n",
    "                self.generative_model_.load_weights(self.generative_network.sr_weights_path)\n",
    "                print(\"Generator weights loaded.\")\n",
    "            except:\n",
    "                print(\"Could not load generator weights.\")\n",
    "\n",
    "        if load_discriminator_weights:\n",
    "            try:\n",
    "                self.discriminative_network.load_gan_weights(self.srgan_model_)\n",
    "                print(\"Discriminator weights loaded.\")\n",
    "            except:\n",
    "                print(\"Could not load discriminator weights.\")\n",
    "\n",
    "        datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        img_width = self.img_width * 4\n",
    "        img_height = self.img_height * 4\n",
    "\n",
    "        early_stop = False\n",
    "        iteration = 0\n",
    "        prev_improvement = -1\n",
    "\n",
    "        if save_loss:\n",
    "            if pre_train_srgan:\n",
    "                loss_history = {'generator_loss' : [],\n",
    "                                'val_psnr' : [], }\n",
    "            elif pre_train_discriminator:\n",
    "                loss_history = {'discriminator_loss' : [],\n",
    "                                'discriminator_acc' : [], }\n",
    "            else:\n",
    "                loss_history = {'discriminator_loss' : [],\n",
    "                                'discriminator_acc' : [],\n",
    "                                'generator_loss' : [],\n",
    "                                'val_psnr': [], }\n",
    "\n",
    "        y_vgg_dummy = np.zeros((self.batch_size * 2, 3, img_width // 32, img_height // 32)) # 5 Max Pools = 2 ** 5 = 32\n",
    "\n",
    "        print(\"Training SRGAN network\")\n",
    "        for i in range(nb_epochs):\n",
    "            print()\n",
    "            print(\"Epoch : %d\" % (i + 1))\n",
    "\n",
    "            for x in datagen.flow_from_directory(image_dir, class_mode=None, batch_size=self.batch_size,\n",
    "                                                 target_size=(img_width, img_height)):\n",
    "                try:\n",
    "                    t1 = time.time()\n",
    "\n",
    "                    if not pre_train_srgan and not pre_train_discriminator:\n",
    "                        x_vgg = x.copy() * 255 # VGG input [0 - 255 scale]\n",
    "\n",
    "                    # resize images\n",
    "                    x_temp = x.copy()\n",
    "                    x_temp = x_temp.transpose((0, 2, 3, 1))\n",
    "\n",
    "                    x_generator = np.empty((self.batch_size, self.img_width, self.img_height, 3))\n",
    "\n",
    "                    for j in range(self.batch_size):\n",
    "                        img = gaussian_filter(x_temp[j], sigma=0.1)\n",
    "                        img = imresize(img, (self.img_width, self.img_height), interp='bicubic')\n",
    "                        x_generator[j, :, :, :] = img\n",
    "\n",
    "                    x_generator = x_generator.transpose((0, 3, 1, 2))\n",
    "\n",
    "                    if iteration % 50 == 0 and iteration != 0 and not pre_train_discriminator:\n",
    "                        print(\"Validation image..\")\n",
    "                        output_image_batch = self.generative_network.get_generator_output(x_generator,\n",
    "                                                                                          self.srgan_model_)\n",
    "                        if type(output_image_batch) == list:\n",
    "                            output_image_batch = output_image_batch[0]\n",
    "\n",
    "                        mean_axis = (0, 2, 3) if K.image_dim_ordering() == 'th' else (0, 1, 2)\n",
    "\n",
    "                        average_psnr = 0.0\n",
    "\n",
    "                        print('gen img mean :', np.mean(output_image_batch / 255., axis=mean_axis))\n",
    "                        print('val img mean :', np.mean(x, axis=mean_axis))\n",
    "\n",
    "                        for x_i in range(self.batch_size):\n",
    "                            average_psnr += psnr(x[x_i], np.clip(output_image_batch[x_i], 0, 255) / 255.)\n",
    "\n",
    "                        average_psnr /= self.batch_size\n",
    "\n",
    "                        if save_loss:\n",
    "                            loss_history['val_psnr'].append(average_psnr)\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "\n",
    "                        print(\"Time required : %0.2f. Average validation PSNR over %d samples = %0.2f\" %\n",
    "                              (t2 - t1, self.batch_size, average_psnr))\n",
    "\n",
    "                        for x_i in range(self.batch_size):\n",
    "                            real_path = \"val_images/epoch_%d_iteration_%d_num_%d_real_.png\" % (i + 1, iteration, x_i + 1)\n",
    "                            generated_path = \"val_images/epoch_%d_iteration_%d_num_%d_generated.png\" % (i + 1,\n",
    "                                                                                                        iteration,\n",
    "                                                                                                        x_i + 1)\n",
    "\n",
    "                            val_x = x[x_i].copy() * 255.\n",
    "                            val_x = val_x.transpose((1, 2, 0))\n",
    "                            val_x = np.clip(val_x, 0, 255).astype('uint8')\n",
    "\n",
    "                            output_image = output_image_batch[x_i]\n",
    "                            output_image = output_image.transpose((1, 2, 0))\n",
    "                            output_image = np.clip(output_image, 0, 255).astype('uint8')\n",
    "\n",
    "                            imsave(real_path, val_x)\n",
    "                            imsave(generated_path, output_image)\n",
    "\n",
    "                        '''\n",
    "                        Don't train of validation images for now.\n",
    "\n",
    "                        Note that if nb_epochs > 1, there is a chance that\n",
    "                        validation images may be used for training purposes as well.\n",
    "\n",
    "                        In that case, this isn't strictly a validation measure, instead of\n",
    "                        just a check to see what the network has learned.\n",
    "                        '''\n",
    "                        continue\n",
    "\n",
    "                    if pre_train_srgan:\n",
    "                        # Train only generator + vgg network\n",
    "\n",
    "                        # Use custom bypass_fit to bypass the check for same input and output batch size\n",
    "                        hist = bypass_fit(self.srgan_model_, [x_generator, x * 255], y_vgg_dummy,\n",
    "                                                     batch_size=self.batch_size, nb_epoch=1, verbose=0)\n",
    "                        sr_loss = hist.history['loss'][0]\n",
    "\n",
    "                        if save_loss:\n",
    "                            loss_history['generator_loss'].extend(hist.history['loss'])\n",
    "\n",
    "                        if prev_improvement == -1:\n",
    "                            prev_improvement = sr_loss\n",
    "\n",
    "                        improvement = (prev_improvement - sr_loss) / prev_improvement * 100\n",
    "                        prev_improvement = sr_loss\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "\n",
    "                        print(\"Iter : %d / %d | Improvement : %0.2f percent | Time required : %0.2f seconds | \"\n",
    "                              \"Generative Loss : %0.2f\" % (iteration, nb_images, improvement, t2 - t1, sr_loss))\n",
    "                    elif pre_train_discriminator:\n",
    "                        # Train only discriminator\n",
    "                        X_pred = self.generative_model_.predict(x_generator, self.batch_size)\n",
    "\n",
    "                        X = np.concatenate((X_pred, x * 255))\n",
    "\n",
    "                        # Using soft and noisy labels\n",
    "                        if np.random.uniform() > disc_train_flip:\n",
    "                            # give correct classifications\n",
    "                            y_gan = [0] * self.batch_size + [1] * self.batch_size\n",
    "                        else:\n",
    "                            # give wrong classifications (noisy labels)\n",
    "                            y_gan = [1] * self.batch_size + [0] * self.batch_size\n",
    "\n",
    "                        y_gan = np.asarray(y_gan, dtype=np.int).reshape(-1, 1)\n",
    "                        y_gan = to_categorical(y_gan, nb_classes=2)\n",
    "                        y_gan = smooth_gan_labels(y_gan)\n",
    "\n",
    "                        hist = self.discriminative_model_.fit(X, y_gan, batch_size=self.batch_size,\n",
    "                                                              nb_epoch=1, verbose=0)\n",
    "\n",
    "                        discriminator_loss = hist.history['loss'][-1]\n",
    "                        discriminator_acc = hist.history['acc'][-1]\n",
    "\n",
    "                        if save_loss:\n",
    "                            loss_history['discriminator_loss'].extend(hist.history['loss'])\n",
    "                            loss_history['discriminator_acc'].extend(hist.history['acc'])\n",
    "\n",
    "                        if prev_improvement == -1:\n",
    "                            prev_improvement = discriminator_loss\n",
    "\n",
    "                        improvement = (prev_improvement - discriminator_loss) / prev_improvement * 100\n",
    "                        prev_improvement = discriminator_loss\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "\n",
    "                        print(\"Iter : %d / %d | Improvement : %0.2f percent | Time required : %0.2f seconds | \"\n",
    "                            \"Discriminator Loss / Acc : %0.4f / %0.2f\" % (iteration, nb_images,\n",
    "                                                            improvement, t2 - t1,\n",
    "                                                            discriminator_loss, discriminator_acc))\n",
    "\n",
    "                    else:\n",
    "                        # Train only discriminator, disable training of srgan\n",
    "                        self.discriminative_network.set_trainable(self.srgan_model_, value=True)\n",
    "                        self.generative_network.set_trainable(self.srgan_model_, value=False)\n",
    "\n",
    "                        # Use custom bypass_fit to bypass the check for same input and output batch size\n",
    "                        # hist = bypass_fit(self.srgan_model_, [x_generator, x * 255, x_vgg],\n",
    "                        #                          [y_gan, y_vgg_dummy],\n",
    "                        #                          batch_size=self.batch_size, nb_epoch=1, verbose=0)\n",
    "\n",
    "                        X_pred = self.generative_model_.predict(x_generator, self.batch_size)\n",
    "\n",
    "                        X = np.concatenate((X_pred, x * 255))\n",
    "\n",
    "                        # Using soft and noisy labels\n",
    "                        if np.random.uniform() > disc_train_flip:\n",
    "                            # give correct classifications\n",
    "                            y_gan = [0] * self.batch_size + [1] * self.batch_size\n",
    "                        else:\n",
    "                            # give wrong classifications (noisy labels)\n",
    "                            y_gan = [1] * self.batch_size + [0] * self.batch_size\n",
    "\n",
    "                        y_gan = np.asarray(y_gan, dtype=np.int).reshape(-1, 1)\n",
    "                        y_gan = to_categorical(y_gan, nb_classes=2)\n",
    "                        y_gan = smooth_gan_labels(y_gan)\n",
    "\n",
    "                        hist1 = self.discriminative_model_.fit(X, y_gan, verbose=0, batch_size=self.batch_size,\n",
    "                                                              nb_epoch=1)\n",
    "\n",
    "                        discriminator_loss = hist1.history['loss'][-1]\n",
    "\n",
    "                        # Train only generator, disable training of discriminator\n",
    "                        self.discriminative_network.set_trainable(self.srgan_model_, value=False)\n",
    "                        self.generative_network.set_trainable(self.srgan_model_, value=True)\n",
    "\n",
    "                        # Using soft labels\n",
    "                        y_model = [1] * self.batch_size\n",
    "                        y_model = np.asarray(y_model, dtype=np.int).reshape(-1, 1)\n",
    "                        y_model = to_categorical(y_model, nb_classes=2)\n",
    "                        y_model = smooth_gan_labels(y_model)\n",
    "\n",
    "                        # Use custom bypass_fit to bypass the check for same input and output batch size\n",
    "                        hist2 = bypass_fit(self.srgan_model_, [x_generator, x, x_vgg], [y_model, y_vgg_dummy],\n",
    "                                           batch_size=self.batch_size, nb_epoch=1, verbose=0)\n",
    "\n",
    "                        generative_loss = hist2.history['loss'][0]\n",
    "\n",
    "                        if save_loss:\n",
    "                            loss_history['discriminator_loss'].extend(hist1.history['loss'])\n",
    "                            loss_history['discriminator_acc'].extend(hist1.history['acc'])\n",
    "                            loss_history['generator_loss'].extend(hist2.history['loss'])\n",
    "\n",
    "                        if prev_improvement == -1:\n",
    "                            prev_improvement = discriminator_loss\n",
    "\n",
    "                        improvement = (prev_improvement - discriminator_loss) / prev_improvement * 100\n",
    "                        prev_improvement = discriminator_loss\n",
    "\n",
    "                        iteration += self.batch_size\n",
    "                        t2 = time.time()\n",
    "                        print(\"Iter : %d / %d | Improvement : %0.2f percent | Time required : %0.2f seconds | \"\n",
    "                              \"Discriminator Loss : %0.3f | Generative Loss : %0.3f\" %\n",
    "                              (iteration, nb_images, improvement, t2 - t1, discriminator_loss, generative_loss))\n",
    "\n",
    "                    if iteration % 1000 == 0 and iteration != 0:\n",
    "                        print(\"Saving model weights.\")\n",
    "                        # Save predictive (SR network) weights\n",
    "                        self._save_model_weights(pre_train_srgan, pre_train_discriminator)\n",
    "                        self._save_loss_history(loss_history, pre_train_srgan, pre_train_discriminator, save_loss)\n",
    "\n",
    "                    if iteration >= nb_images:\n",
    "                        break\n",
    "\n",
    "                except KeyboardInterrupt:\n",
    "                    print(\"Keyboard interrupt detected. Stopping early.\")\n",
    "                    early_stop = True\n",
    "                    break\n",
    "\n",
    "            iteration = 0\n",
    "\n",
    "            if early_stop:\n",
    "                break\n",
    "\n",
    "        print(\"Finished training SRGAN network. Saving model weights.\")\n",
    "        # Save predictive (SR network) weights\n",
    "        self._save_model_weights(pre_train_srgan, pre_train_discriminator)\n",
    "        self._save_loss_history(loss_history, pre_train_srgan, pre_train_discriminator, save_loss)\n",
    "\n",
    "    def _save_model_weights(self, pre_train_srgan, pre_train_discriminator):\n",
    "        if not pre_train_discriminator:\n",
    "            self.generative_model_.save_weights(self.generative_network.sr_weights_path, overwrite=True)\n",
    "\n",
    "        if not pre_train_srgan:\n",
    "            # Save GAN (discriminative network) weights\n",
    "            self.discriminative_network.save_gan_weights(self.discriminative_model_)\n",
    "\n",
    "    def _save_loss_history(self, loss_history, pre_train_srgan, pre_train_discriminator, save_loss):\n",
    "        if save_loss:\n",
    "            print(\"Saving loss history\")\n",
    "\n",
    "            if pre_train_srgan:\n",
    "                with open('pretrain losses - srgan.json', 'w') as f:\n",
    "                    json.dump(loss_history, f)\n",
    "            elif pre_train_discriminator:\n",
    "                with open('pretrain losses - discriminator.json', 'w') as f:\n",
    "                    json.dump(loss_history, f)\n",
    "            else:\n",
    "                with open('fulltrain losses.json', 'w') as f:\n",
    "                    json.dump(loss_history, f)\n",
    "\n",
    "            print(\"Saved loss history\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from keras.utils.visualize_util import plot\n",
    "\n",
    "    # Path to MS COCO dataset\n",
    "    coco_path = r\"D:\\Yue\\Documents\\Dataset\\coco2014\\train2014\"\n",
    "\n",
    "    '''\n",
    "    Base Network manager for the SRGAN model\n",
    "\n",
    "    Width / Height = 32 to reduce the memory requirement for the discriminator.\n",
    "\n",
    "    Batch size = 1 is slower, but uses the least amount of gpu memory, and also acts as\n",
    "    Instance Normalization (batch norm with 1 input image) which speeds up training slightly.\n",
    "    '''\n",
    "\n",
    "    srgan_network = SRGANNetwork(img_width=32, img_height=32, batch_size=1)\n",
    "    srgan_network.build_srgan_model()\n",
    "    #plot(srgan_network.srgan_model_, 'SRGAN.png', show_shapes=True)\n",
    "\n",
    "    # Pretrain the SRGAN network\n",
    "    #srgan_network.pre_train_srgan(coco_path, nb_images=80000, nb_epochs=1)\n",
    "\n",
    "    # Pretrain the discriminator network\n",
    "    #srgan_network.pre_train_discriminator(coco_path, nb_images=40000, nb_epochs=1, batch_size=16)\n",
    "\n",
    "    # Fully train the SRGAN with VGG loss and Discriminator loss\n",
    "    srgan_network.train_full_model(coco_path, nb_images=80000, nb_epochs=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
